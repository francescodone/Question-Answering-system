{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEAk3Ts9FytP"
      },
      "source": [
        "# DIKU NLP Course 2020/2021: Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQVVe8FWy_pS"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAD9srW2kwH"
      },
      "source": [
        "#### Mount Google Drive (datasets are stored there):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP4L-yFeYURJ",
        "outputId": "d19ae2b6-d69c-43e6-af07-24c3a4ecdfd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-lmFXoGyvc9"
      },
      "source": [
        "#### Enable reproducability\n",
        "\n",
        "Taken from https://nbviewer.jupyter.org/github/copenlu/stat-nlp-book/blob/master/labs/lab_2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p94XilAyvdT"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def enforce_reproducibility(seed=42):\n",
        "  # Sets seed manually for both CPU and CUDA\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # For atomic operations there is currently no simple way to enforce \n",
        "  # determinism, as the order of parallel operations is not known.\n",
        "  # CUDNN\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "  # System based\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtaVl79pzm54"
      },
      "source": [
        "#### Load relevant questions from dataset, also download nltk word tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7lUS_on3q1x",
        "outputId": "c94f1c2b-2f2a-4652-ba35-123a71e89605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# we use nltk to tokenize multi-lingual sequences\n",
        "def tokenize_at_word_level(input):\n",
        "  return nltk.tokenize.word_tokenize(input)\n",
        "\n",
        "# define supported languages\n",
        "supported_languages = ['english', 'arabic', 'finnish', 'korean']\n",
        "\n",
        "binary_labels = ['YES', 'NO']\n",
        "\n",
        "# helper function to return all relevant properties\n",
        "def relevant_properties(question):\n",
        "  return {\n",
        "    \"question\": question['question_text'],\n",
        "    \"document\": question['document_plaintext'],\n",
        "    \"answer\": question['annotations'][0]['yes_no_answer'].upper()\n",
        "  }\n",
        "\n",
        "# helper function to import questions from given file\n",
        "def import_questions(file):\n",
        "  questions = {}\n",
        "\n",
        "  for lang in supported_languages:\n",
        "    questions[lang] = []\n",
        "\n",
        "  for line in file:\n",
        "    question = json.loads(line)\n",
        "    lang = question['language']\n",
        "\n",
        "    # add question if dict contains key for it and it has yes/no answer \n",
        "    if (lang in list(questions.keys()) and\n",
        "        relevant_properties(question)['answer'] in binary_labels\n",
        "      ):\n",
        "      questions[lang].append(question)\n",
        "\n",
        "  return questions\n",
        "\n",
        "# questions used for training our classifier(s)\n",
        "with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-v1.0-train.jsonl\") as file:\n",
        "  train_questions = import_questions(file)\n",
        "\n",
        "# questions used to evaluate our classifier(s)\n",
        "with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-v1.0-dev.jsonl\") as file:\n",
        "  dev_questions = import_questions(file)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEWPZkmUyUwa"
      },
      "source": [
        "## 1 Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW7f_D--uQ2E"
      },
      "source": [
        "### 1.1 Preprocessing and dataset analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4mdb3fRzW71"
      },
      "source": [
        "#### (a) Preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3bHLYCrzeF4",
        "outputId": "6f789133-1f38-42df-b5db-cebc45857562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenize_at_word_level(\"This method may be used to tokenize sentences.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'method', 'may', 'be', 'used', 'to', 'tokenize', 'sentences', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcT6SP5EkkZa"
      },
      "source": [
        "#### (b) Most common first tokens and common question words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYWtOsgSz1Hw",
        "outputId": "a14572b7-1ed0-4c33-d1d1-c22c2b21a319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def first_tokens(questions):\n",
        "  first_tokens = {}\n",
        "\n",
        "  # store counter for each first token within dictionary\n",
        "  for lang in list(questions.keys()):\n",
        "    if lang not in first_tokens:\n",
        "      first_tokens[lang] = {}\n",
        "    for question in questions[lang]:\n",
        "      token = tokenize_at_word_level(question['question_text'])[0]\n",
        "      if token in first_tokens[lang]:\n",
        "        first_tokens[lang][token] += 1\n",
        "      else:\n",
        "        first_tokens[lang][token] = 1\n",
        "\n",
        "  return first_tokens\n",
        "\n",
        "first_tokens(train_questions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arabic': {'أصبحت': 1,\n",
              "  'الستيرويد': 1,\n",
              "  'ما': 1,\n",
              "  'متى': 1,\n",
              "  'من': 2,\n",
              "  'نشأت': 1,\n",
              "  'هل': 1369,\n",
              "  'هي': 3,\n",
              "  'يشجع': 1},\n",
              " 'english': {'Are': 58,\n",
              "  'Can': 49,\n",
              "  'Did': 65,\n",
              "  'Do': 49,\n",
              "  'Does': 74,\n",
              "  'Has': 16,\n",
              "  'Have': 3,\n",
              "  'How': 1,\n",
              "  'IS': 1,\n",
              "  'In': 1,\n",
              "  'Is': 201,\n",
              "  'Was': 33,\n",
              "  'Were': 5,\n",
              "  'When': 1,\n",
              "  'Which': 1,\n",
              "  'Will': 3,\n",
              "  'can': 1},\n",
              " 'finnish': {'Aiheuttaako': 4,\n",
              "  'Alentavatko': 1,\n",
              "  'Asui': 1,\n",
              "  'Asuiko': 1,\n",
              "  'Asuivatko': 1,\n",
              "  'Asuuko': 4,\n",
              "  'Asuvatko': 1,\n",
              "  'Auttoiko': 1,\n",
              "  'Elääkö': 8,\n",
              "  'Erikoistuivatko': 1,\n",
              "  'Johtuuko': 1,\n",
              "  'Julistivatko': 1,\n",
              "  'Juontuvatko': 1,\n",
              "  'Järjestetäänkö': 1,\n",
              "  'Kaatuiko': 1,\n",
              "  'Kannattaako': 1,\n",
              "  'Kasvaako': 5,\n",
              "  'Kertooko': 1,\n",
              "  'Kiduttivatko': 1,\n",
              "  'Kiinnitetäänkö': 1,\n",
              "  'Kuinka': 1,\n",
              "  'Kuljettaako': 1,\n",
              "  'Kun': 1,\n",
              "  'Kuoleeko': 1,\n",
              "  'Kuolivatko': 1,\n",
              "  'Kuuluuko': 17,\n",
              "  'Käytettiinkö': 1,\n",
              "  'Käytetäänkö': 3,\n",
              "  'Lasketaanko': 1,\n",
              "  'Laulavatko': 1,\n",
              "  'Liikkuvatko': 1,\n",
              "  'Lopettiko': 1,\n",
              "  'Luetaanko': 1,\n",
              "  'Löytyykö': 1,\n",
              "  'Maksetaanko': 1,\n",
              "  'Mikä': 2,\n",
              "  'Minkä': 1,\n",
              "  'Muniiko': 1,\n",
              "  'Myydäänkö': 1,\n",
              "  'Määräytyykö': 1,\n",
              "  'Näytteleekö': 4,\n",
              "  'Näyttelikö': 4,\n",
              "  'Oliko': 74,\n",
              "  'Olivatko': 3,\n",
              "  'Onko': 775,\n",
              "  'Osallistuiko': 1,\n",
              "  'Osasiko': 1,\n",
              "  'Ovatko': 37,\n",
              "  'Pelaako': 2,\n",
              "  'Pelataanko': 3,\n",
              "  'Perustivatko': 1,\n",
              "  'Perustuuko': 1,\n",
              "  'Pilkkooko': 1,\n",
              "  'Puhutaanko': 1,\n",
              "  'Pystyykö': 2,\n",
              "  'Rakennetaanko': 1,\n",
              "  'Ruskettaako': 1,\n",
              "  'Saako': 2,\n",
              "  'Saavatko': 1,\n",
              "  'Saiko': 1,\n",
              "  'Saivatko': 1,\n",
              "  'Sijaitseeko': 4,\n",
              "  'Sisältääkö': 5,\n",
              "  'Sodittiinko': 1,\n",
              "  'Sotivatko': 1,\n",
              "  'Synnyttääkö': 1,\n",
              "  'Syntyykö': 2,\n",
              "  'Syödäänkö': 1,\n",
              "  'Syökö': 1,\n",
              "  'Syövätkö': 3,\n",
              "  'Taisteleeko': 1,\n",
              "  'Tappoivatko': 1,\n",
              "  'Tehdäänkö': 1,\n",
              "  'Toimiiko': 1,\n",
              "  'Tuhosivatko': 1,\n",
              "  'Tukivatko': 1,\n",
              "  'Tuleeko': 1,\n",
              "  'Tuotetaanko': 1,\n",
              "  'Turpoavatko': 1,\n",
              "  'Valmistaako': 1,\n",
              "  'Virtaako': 2,\n",
              "  'Voikko': 1,\n",
              "  'Voiko': 49,\n",
              "  'Voisiko': 1,\n",
              "  'Vähentääkö': 2},\n",
              " 'korean': {'1215년': 1,\n",
              "  '16세기': 1,\n",
              "  '1906': 1,\n",
              "  '1914년': 1,\n",
              "  '1986년에는': 1,\n",
              "  '1991년': 1,\n",
              "  '2D의': 1,\n",
              "  'DNA는': 1,\n",
              "  'GPU는': 1,\n",
              "  'HIV를': 1,\n",
              "  'LMX이론은': 1,\n",
              "  'MTV가': 1,\n",
              "  'PSI는': 1,\n",
              "  'PowerVR': 1,\n",
              "  'R/S': 1,\n",
              "  'RNA는': 1,\n",
              "  '가부장제란': 1,\n",
              "  '가톨릭': 1,\n",
              "  '가톨릭교회의': 4,\n",
              "  '감정': 1,\n",
              "  '감정표현규칙은': 1,\n",
              "  '거성은': 1,\n",
              "  '거울상': 2,\n",
              "  '경동교회는': 1,\n",
              "  '계보학은': 1,\n",
              "  '고대': 1,\n",
              "  '공기는': 1,\n",
              "  '공동소송인': 1,\n",
              "  '관도': 1,\n",
              "  '교토는': 1,\n",
              "  '구리는': 1,\n",
              "  '구약성경의': 1,\n",
              "  '구조': 1,\n",
              "  '구지가는': 1,\n",
              "  '균류는': 1,\n",
              "  '그래핀의': 2,\n",
              "  '그리스는': 1,\n",
              "  '그리스어는': 1,\n",
              "  '극초신성은': 1,\n",
              "  '근삿값에': 1,\n",
              "  '근육에': 1,\n",
              "  '금속결합은': 1,\n",
              "  '금속은': 1,\n",
              "  '금은': 1,\n",
              "  '기독교는': 1,\n",
              "  '기록된': 1,\n",
              "  '기수': 1,\n",
              "  '기업결합은': 1,\n",
              "  '기화식': 1,\n",
              "  '난민의': 1,\n",
              "  '날다람쥐가': 1,\n",
              "  '노아는': 1,\n",
              "  '뇌와': 1,\n",
              "  '뇌졸중이': 1,\n",
              "  '다른': 1,\n",
              "  '다운': 1,\n",
              "  '다이아몬드는': 1,\n",
              "  '닥터': 1,\n",
              "  '단당류는': 1,\n",
              "  '단종은': 1,\n",
              "  '담배에': 1,\n",
              "  '대뇌는': 1,\n",
              "  '대법원의': 1,\n",
              "  '대폭발은': 1,\n",
              "  '대한민국은': 1,\n",
              "  '덴마크는': 2,\n",
              "  '덴마크의': 1,\n",
              "  '덴포의': 1,\n",
              "  '도마뱀은': 1,\n",
              "  '도미니카': 1,\n",
              "  '도솔가는': 1,\n",
              "  '독일은': 1,\n",
              "  '독일의': 1,\n",
              "  '독점규제법에서는': 2,\n",
              "  '돌고래는': 1,\n",
              "  '동양에서는': 1,\n",
              "  '두툽상어는': 1,\n",
              "  '디젤': 2,\n",
              "  '라소는': 1,\n",
              "  '러시안': 1,\n",
              "  '렘': 1,\n",
              "  '렙톤은': 2,\n",
              "  '로마': 1,\n",
              "  '로마는': 1,\n",
              "  '로마의': 1,\n",
              "  '리투아니아는': 3,\n",
              "  '리튬이온전지': 1,\n",
              "  '리튬이온전지는': 1,\n",
              "  '마르크스-레닌주의에서는': 1,\n",
              "  '마리화나는': 1,\n",
              "  '마이크로소프트': 1,\n",
              "  '마켓': 1,\n",
              "  '말레이시아에서는': 1,\n",
              "  '모든': 2,\n",
              "  '모빌슈트는': 1,\n",
              "  '목성은': 1,\n",
              "  '무슬림은': 2,\n",
              "  '무중량상태': 1,\n",
              "  '무함마드': 1,\n",
              "  '미국은': 2,\n",
              "  '미림': 1,\n",
              "  '미토콘드리아는': 2,\n",
              "  '미토콘드리아에는': 1,\n",
              "  '미토콘드리아의': 1,\n",
              "  '민주주의의': 1,\n",
              "  '바빌론은': 1,\n",
              "  '바빌론의': 1,\n",
              "  '박피': 1,\n",
              "  '반증이': 1,\n",
              "  '반환': 1,\n",
              "  '발효는': 1,\n",
              "  '버섯은': 1,\n",
              "  '버스': 1,\n",
              "  '베드로는': 1,\n",
              "  '베트남은': 1,\n",
              "  '부커': 1,\n",
              "  '불교에서': 1,\n",
              "  '브라질에서도': 1,\n",
              "  '브리튼': 1,\n",
              "  '브릿': 1,\n",
              "  '비브라늄이': 1,\n",
              "  '사냥의': 1,\n",
              "  '사회주의는': 1,\n",
              "  '산스크리트어는': 2,\n",
              "  '상시': 1,\n",
              "  '서양': 1,\n",
              "  '성골과': 1,\n",
              "  '성공회는': 1,\n",
              "  '성리학을': 1,\n",
              "  '세포막은': 1,\n",
              "  '세포의': 1,\n",
              "  '셀룰로오스는': 1,\n",
              "  '소금은': 1,\n",
              "  '소리의': 1,\n",
              "  '소프트웨어': 1,\n",
              "  '손빈병법은': 1,\n",
              "  '손빈은': 1,\n",
              "  '쇼와': 1,\n",
              "  '순수과학은': 1,\n",
              "  '스베티츠호벨리': 1,\n",
              "  '스웨덴에': 1,\n",
              "  '스칸디나비아에서는': 1,\n",
              "  '스케테의': 1,\n",
              "  '스케티스': 1,\n",
              "  '스코틀랜드는': 1,\n",
              "  '스코틀랜드에는': 1,\n",
              "  '스페인': 1,\n",
              "  '스피노자에게': 1,\n",
              "  '식물은': 2,\n",
              "  '신구약': 1,\n",
              "  '신사는': 1,\n",
              "  '실재주의': 1,\n",
              "  '심리극은': 1,\n",
              "  '심리극의': 1,\n",
              "  '십자군은': 1,\n",
              "  '아서왕': 1,\n",
              "  '아우토슈타트는': 1,\n",
              "  '아우토슈타트에는': 1,\n",
              "  '아프가니스탄은': 1,\n",
              "  '알렉산드르': 1,\n",
              "  '알루미늄은': 1,\n",
              "  '암사자들은': 1,\n",
              "  '양산시는': 1,\n",
              "  '엔트로피는': 1,\n",
              "  '엽록체는': 1,\n",
              "  '영국에서': 1,\n",
              "  '영국은': 1,\n",
              "  '영국의': 1,\n",
              "  '영어의': 1,\n",
              "  '영의정은': 1,\n",
              "  '예쁜꼬마선충은': 3,\n",
              "  '오늘날의': 1,\n",
              "  '오세트인은': 1,\n",
              "  '오쇼가쓰': 1,\n",
              "  '오페라': 1,\n",
              "  '오페라는': 1,\n",
              "  '오픈스택은': 1,\n",
              "  '오행이라는': 2,\n",
              "  '온도의': 1,\n",
              "  '온타리오주는': 1,\n",
              "  '왜구란': 1,\n",
              "  '외계': 1,\n",
              "  '용수철이란': 1,\n",
              "  '용은': 2,\n",
              "  '용의': 2,\n",
              "  '우주에': 1,\n",
              "  '움직임': 1,\n",
              "  '워싱턴': 1,\n",
              "  '워치독': 1,\n",
              "  '월드': 1,\n",
              "  '위석': 1,\n",
              "  '윌리엄': 1,\n",
              "  '유대교': 1,\n",
              "  '율령에서': 1,\n",
              "  '은나라는': 1,\n",
              "  '은은': 1,\n",
              "  '이마누엘': 1,\n",
              "  '이슬람교와': 1,\n",
              "  '이슬람은': 1,\n",
              "  '이시애는': 1,\n",
              "  '인권은': 1,\n",
              "  '인도는': 3,\n",
              "  '인도에': 1,\n",
              "  '인동': 1,\n",
              "  '인터넷은': 1,\n",
              "  '일반상대성이론에서': 1,\n",
              "  '일본서기는': 1,\n",
              "  '일본에서는': 1,\n",
              "  '일본은': 2,\n",
              "  '자연주의는': 1,\n",
              "  '자폐증은': 1,\n",
              "  '재건된': 1,\n",
              "  '재즈는': 1,\n",
              "  '적색편이는': 1,\n",
              "  '전': 1,\n",
              "  '전류의': 1,\n",
              "  '점성술은': 1,\n",
              "  '정보': 1,\n",
              "  '정적': 1,\n",
              "  '제3궤조': 1,\n",
              "  '조선은': 1,\n",
              "  '조정래의': 1,\n",
              "  '주식회사는': 1,\n",
              "  '중국어는': 1,\n",
              "  '중세는': 1,\n",
              "  '중화민국은': 1,\n",
              "  '지구': 1,\n",
              "  '지구의': 1,\n",
              "  '지온': 1,\n",
              "  '질량의': 1,\n",
              "  '찰스': 1,\n",
              "  '창세기는': 2,\n",
              "  '채권관계에서': 1,\n",
              "  '채무': 1,\n",
              "  '척추동물의': 1,\n",
              "  '초록색의': 1,\n",
              "  '초전도는': 1,\n",
              "  '촉매': 2,\n",
              "  '총리대신': 1,\n",
              "  '커피': 1,\n",
              "  '코뿔소는': 1,\n",
              "  '코일': 1,\n",
              "  '코트니': 1,\n",
              "  '타이머는': 1,\n",
              "  '태양계의': 1,\n",
              "  '태양은': 2,\n",
              "  '태풍으': 1,\n",
              "  '터키': 1,\n",
              "  '토종벌낭충봉아부패병의': 1,\n",
              "  '파리는': 1,\n",
              "  '판테온': 1,\n",
              "  '페르시아인은': 1,\n",
              "  '포틀랜드는': 1,\n",
              "  '폴란드는': 2,\n",
              "  '프랑스어는': 1,\n",
              "  '프리드리히': 1,\n",
              "  '피에르': 1,\n",
              "  '하늘의': 1,\n",
              "  '하와이주는': 1,\n",
              "  '하이텔은': 1,\n",
              "  '한국': 1,\n",
              "  '한국어': 1,\n",
              "  '한국에서': 2,\n",
              "  '한국에서는': 2,\n",
              "  '한국은': 3,\n",
              "  '한국의': 2,\n",
              "  '한자는': 1,\n",
              "  '헤라클레스는': 1,\n",
              "  '현대의': 1,\n",
              "  '혜성의': 1,\n",
              "  '호랑이는': 1,\n",
              "  '홍콩은': 1,\n",
              "  '화랑도의': 1,\n",
              "  '화투는': 1,\n",
              "  '확률이': 1,\n",
              "  '흡연이': 1}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhPkQe9Yyhgr"
      },
      "source": [
        "### 1.2 Binary Question Classification\n",
        "\n",
        "We chose to go with a simple logistic regression model, the input being a concatenation of the question and document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n36NvqwF72oc"
      },
      "source": [
        "# return number of occurrences of words as feature dictionary\n",
        "def features(text, lang):\n",
        "  features = defaultdict(float)\n",
        "  for w in tokenize_at_word_level(text):\n",
        "    features[w] += 1.0\n",
        "  return features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqz4kkYU7uEv"
      },
      "source": [
        "# train logistic regression classifier\n",
        "def train_binary_log_reg(lang):\n",
        "  train_data = [relevant_properties(q) for q in train_questions[lang]]\n",
        "  dev_data = [relevant_properties(q) for q in dev_questions[lang]]\n",
        "\n",
        "  vectorizer = DictVectorizer()\n",
        "\n",
        "  # we again use the concatenated question and document text as features\n",
        "  train_x = vectorizer.fit_transform([features(q['question'] + \" \" + q['document'], lang) for q in train_data])\n",
        "  dev_x = vectorizer.transform([features(q['question'] + \" \" + q['document'], lang) for q in dev_data])\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  train_y = label_encoder.fit_transform([q['answer'] for q in train_data])\n",
        "  dev_y = label_encoder.fit_transform([q['answer'] for q in dev_data])\n",
        "\n",
        "  lr = LogisticRegression(C=1000, penalty=\"l1\", random_state=1, solver='liblinear')\n",
        "  lr.fit(train_x, train_y)\n",
        "\n",
        "  # inverse_transform transforms labels back to original encoding\n",
        "  return label_encoder.inverse_transform(lr.predict(dev_x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEhzbn2r5ld",
        "outputId": "0d560fc4-d4f4-4d67-fa15-11a839dd325f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "%%time\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# train binary classifier for all languages and evaluate the predictions\n",
        "for lang in supported_languages:\n",
        "  predictions = train_binary_log_reg(lang)\n",
        "  actual = [relevant_properties(q)['answer'] for q in dev_questions[lang]]\n",
        "\n",
        "  print('Accuracy for language {}: {}'.format(lang, accuracy_score(actual, predictions)))\n",
        "  print('F1 score for language {}: {}'.format(lang, f1_score(actual, predictions, average='weighted')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for language english: 0.5194805194805194\n",
            "F1 score for language english: 0.503806081787733\n",
            "Accuracy for language arabic: 0.813953488372093\n",
            "F1 score for language arabic: 0.813953488372093\n",
            "Accuracy for language finnish: 0.723404255319149\n",
            "F1 score for language finnish: 0.6412424813717568\n",
            "Accuracy for language korean: 0.9354838709677419\n",
            "F1 score for language korean: 0.9043010752688171\n",
            "CPU times: user 1min 35s, sys: 409 ms, total: 1min 35s\n",
            "Wall time: 1min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}