{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week3_working.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHoYwd73k_jb",
        "outputId": "5ec89d6b-6ae1-4b6c-ef9a-e92ea37b7ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Hho2HYlEzf"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def enforce_reproducibility(seed=42):\n",
        "  # Sets seed manually for both CPU and CUDA\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # For atomic operations there is currently no simple way to enforce \n",
        "  # determinism, as the order of parallel operations is not known.\n",
        "  # CUDNN\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "  # System based\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuuUcDB4lH0N"
      },
      "source": [
        "# read_data.py\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class TxtEntryReader(HTMLParser):\n",
        "    # this class reads tags in txt files\n",
        "    # formatted as in trans_data.py\n",
        "    entries = []\n",
        "    entry = {}\n",
        "    curr_tag = \"\"\n",
        "\n",
        "    def handle_starttag(self,tag,attrs):\n",
        "        self.curr_tag = tag\n",
        "\n",
        "    def handle_endtag(self,tag):\n",
        "        self.curr_tag = \"\"\n",
        "        if tag == \"entry\":\n",
        "            self.entries.append(self.entry)\n",
        "            self.entry = {}\n",
        "\n",
        "    def handle_data(self,data):\n",
        "        if self.curr_tag != \"\" and self.curr_tag != \"entry\":\n",
        "            self.entry[self.curr_tag] = data.strip()\n",
        "\n",
        "\n",
        "def get_data_2(fname):\n",
        "    # returns a list of entry-records\n",
        "    parser = TxtEntryReader()\n",
        "    with open(\"/content/drive/My Drive/NLP 2020W/data-txt/\" + fname,\"r\") as f:\n",
        "        d0 = f.read()\n",
        "        parser.feed(d0)\n",
        "    return parser.entries\n",
        "\n",
        "def get_size(fname):\n",
        "    return len(get_data_2(fname))\n",
        "\n",
        "# we are interested in the fields\n",
        "#d0 = get_data(\"train-eng.txt\")\n",
        "#print(d0[0][\"question_txt\"])\n",
        "#print(d0[0][\"doc_txt\"])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWr1JPvnmkgN",
        "outputId": "a981e6cd-d76e-48a0-f9a8-e5a7d071938d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\r\u001b[K     |▍                               | 10kB 21.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 28.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 11.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 10.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 983kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=c0068fe99dc30c7d6986a02636513368da77d20652c792cbb351004705a612ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da8pv1RElNJW",
        "outputId": "9f911745-7f51-45a0-88e1-8ad53385a441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# trans_data.py\n",
        "\n",
        "import json\n",
        "from langdetect import detect\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "\n",
        "\n",
        "\n",
        "def get_data_lines(s):\n",
        "    retval = []\n",
        "    with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-v1.0-\"+s+\".jsonl\",\"r\") as f:\n",
        "        retval = f.readlines()\n",
        "    return retval\n",
        "\n",
        "def save_txt(fname,txt):\n",
        "    with open(\"/content/drive/My Drive/NLP 2020W/data-txt/\" + fname,\"w\") as f:\n",
        "        f.write(txt)\n",
        "\n",
        "def create_tag(tag_name,tag_txt):\n",
        "    retval  = \"<\" + tag_name + \">\\n\"\n",
        "    retval += tag_txt + \"\\n\"\n",
        "    retval += \"</\" + tag_name + \">\\n\"\n",
        "    return retval\n",
        "\n",
        "\n",
        "def filter_lang(lines,pred):\n",
        "    # filter according to a given predicate\n",
        "    # returns a string formatted with tags instead\n",
        "    # of as a json structure\n",
        "    retval = \"\"\n",
        "    nr_entries = 0\n",
        "    \n",
        "    print(json.loads(lines[0]).keys())\n",
        "\n",
        "    for line in lines:\n",
        "        jline = json.loads(line)\n",
        "        if pred(jline):\n",
        "            nr_entries += 1\n",
        "            res  = create_tag(\"lang\",jline[\"language\"])\n",
        "            res += create_tag(\"title\",jline[\"document_title\"])\n",
        "            res += create_tag(\"question_txt\",jline[\"question_text\"])\n",
        "            #res += create_tag(\"p_answ_cand\",jline[\"passage_answer_candidates\"])\n",
        "            res += create_tag(\"doc_txt\",jline[\"document_plaintext\"])\n",
        "            annon  = create_tag(\"yes_no_answ\",jline[\"annotations\"][0][\"yes_no_answer\"])\n",
        "            annon += create_tag(\"start_byte\",str(jline[\"annotations\"][0][\"minimal_answer\"][\"plaintext_start_byte\"]))\n",
        "            annon += create_tag(\"end_byte\",str(jline[\"annotations\"][0][\"minimal_answer\"][\"plaintext_end_byte\"]))\n",
        "            res += create_tag(\"annon\",annon)\n",
        "            retval += create_tag(\"entry\",res)\n",
        "    print(\"nr entries: \" + str(nr_entries))\n",
        "    return retval\n",
        "\n",
        "def pred1(jline):\n",
        "    langs = {\n",
        "            \"1\":\"english\"\n",
        "            }\n",
        "    return jline[\"language\"] == langs[\"1\"]\n",
        "\n",
        "res = filter_lang(get_data_lines(\"train\"),pred1)\n",
        "\n",
        "save_txt(\"train-eng.txt\",res)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['annotations', 'document_plaintext', 'document_title', 'document_url', 'example_id', 'language', 'passage_answer_candidates', 'question_text'])\n",
            "nr entries: 9211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71dC42L9lQVu",
        "outputId": "7f8a7fe1-52ae-4e64-c457-ba0cff48a799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "# build_model.py\n",
        "\n",
        "# import so we can pause the execution\n",
        "import sys\n",
        "\n",
        "# imports copied from lab3\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam, RMSprop\n",
        "\n",
        "# read data from tag-formatted-files\n",
        "#import read_data\n",
        "\n",
        "# for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# code taken from lab3 notebook:\n",
        "# https://colab.research.google.com/drive/1IeAOa4NWHu8q6sL6Gb54M1TiOWW-4f56?usp=sharing\n",
        "\n",
        "\n",
        "# tokenizers\n",
        "def regex_tokenizer(txt):\n",
        "    # here words are tokenized only\n",
        "    # hence not eg. symbols like ',' or '['\n",
        "    rx_en = \"[a-zA-Z0-9\\\\-]+|[a-zA-Z0-9\\\\-]+'[a-zA-Z0-9\\\\-]*\"\n",
        "    return re.findall(rx_en,txt)\n",
        "\n",
        "def regex_tokenizer_lower(txt):\n",
        "    # the same as above, though all words are lower cased\n",
        "    return [t.lower() for t in regex_tokenizer(txt)]\n",
        "\n",
        "def lab_tokenizer(txt):\n",
        "    retval = []\n",
        "    for line in txt.split(\"\\n\"):\n",
        "        tokens = line.strip().split(\" \")\n",
        "        retval += tokens\n",
        "    return retval\n",
        "\n",
        "def get_data(entries,sizes):\n",
        "    # We just concat question and doc with a space ' ' between\n",
        "    train,test,valid = [],[],[]\n",
        "    for t in entries[0:sizes[0]]:\n",
        "        train.append(t[\"question_txt\"] + \" \" + t[\"doc_txt\"])\n",
        "    for t in entries[sizes[0]:sizes[0] + sizes[1]]:\n",
        "        test.append(t[\"question_txt\"] + \" \" + t[\"doc_txt\"])\n",
        "    for t in entries[sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]:\n",
        "        valid.append(t[\"question_txt\"] + \" \" + t[\"doc_txt\"])\n",
        "    return (train,test,valid)\n",
        "\n",
        "# we always do this\n",
        "def enforce_reproducibility(seed=42):\n",
        "    # Sets seed manually for both CPU and CUDA\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # For atomic operations there is currently \n",
        "    # no simple way to enforce determinism, as\n",
        "    # the order of parallel operations is not known.\n",
        "    # CUDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # System based\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "enforce_reproducibility()\n",
        "\n",
        "\n",
        "# We build a vocab using the py-class from the lab\n",
        "class Vocab:\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = []\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self.word2idx[word] = len(self.idx2word) - 1\n",
        "    return self.word2idx[word]\n",
        "    \n",
        "  def to_words(self, ids):\n",
        "    return [self.idx2word[idx] for idx in ids]\n",
        "\n",
        "  def to_ids(self, words):\n",
        "    return [self.add_word(word) for word in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        "\n",
        "def build_vocab():\n",
        "    vocabulary = Vocab()\n",
        "    vocabulary.add_word('<eos>') # include a token indicating the end of a string\n",
        "    vocabulary.add_word('<pad>') # include a token for padding\n",
        "    vocabulary.add_word(\"<unk>\") # include a token for unknown\n",
        "\n",
        "    for entry in train_data + test_data + valid_data:\n",
        "        # using specialized tokenizer results in less tokens\n",
        "        # since eg. 'Stadium.[1]' is different from 'Stadium'\n",
        "        tokens = tokenize(entry)\n",
        "        for token in tokens:\n",
        "            vocabulary.add_word(token)\n",
        "\n",
        "    print(len(vocabulary))\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "# read in the dataset\n",
        "class TxtDatasetReader(Dataset):\n",
        "    # This reader creates sequences of length of seq_len\n",
        "    def __init__(self, entries: str, vocab: Vocab, seq_len: int):\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.dataset = []\n",
        "\n",
        "        for document in entries:\n",
        "            #tokens = [token for token in document.split(' ') if len(token) > 0]\n",
        "            tokens = tokenize(document)\n",
        "            if len(tokens) <= 1:\n",
        "                # skip empty rows\n",
        "                continue\n",
        "\n",
        "            # iterate through the wiki document with a window of size seq_len\n",
        "            # each window is a separate instance\n",
        "            doc_instances = [tokens[i:i+seq_len] for i in range(0, len(tokens), seq_len)]\n",
        "    \n",
        "            # pad the last window, which can be shorter than seq_len\n",
        "            doc_instances[-1] = doc_instances[-1] + ['<pad>'] * (seq_len - len(doc_instances[-1]))\n",
        "            self.dataset.extend(doc_instances)\n",
        "\n",
        "        # truncate last incomplete batch (the hidden states for the RNN have the shape of the batch)\n",
        "        self.dataset = self.dataset[:(len(self.dataset)-(len(self.dataset) % batch_size))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # return some sequence with idx as index\n",
        "        return self.vocab.to_ids(self.dataset[idx])\n",
        "\n",
        "\n",
        "def collate_batch_bilstm(input_t: List) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Combines multiple data samples into a single batch\n",
        "    :param input_t: The token input ids\n",
        "    :return: A tuple of tensors (input_ids, targets, input_lens)\n",
        "    \"\"\"\n",
        "    input_data = torch.tensor(input_t)\n",
        "\n",
        "    \n",
        "    # we don't use the last position as there isn't anything left for generation\n",
        "    input_ids = input_data[:, :-1]\n",
        "    # the length of every id, used for packing\n",
        "    input_lens = [len(ids) for ids in input_ids]\n",
        "    input_lens = torch.tensor(input_lens)\n",
        "    # the target at each step is to generate the next word from the sequence\n",
        "    # so we shift the token ids with 1 position\n",
        "    targets = input_data[:, 1:]\n",
        "    return input_ids, targets, input_lens\n",
        "\n",
        "# Define the model\n",
        "class LSTMNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic LSTM network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size: int, \n",
        "            embeddings_dim: int,\n",
        "            lstm_dim: int,       \n",
        "            n_words: int,      \n",
        "            dropout_prob: float = 0.0,\n",
        "            lstm_layers: int = 1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializer for basic BiLSTM network\n",
        "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
        "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
        "        :param dropout_prob: Dropout probability\n",
        "        :param n_classes: The number of output classes\n",
        "        \"\"\"\n",
        "\n",
        "        # First thing is to call the superclass initializer\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "\n",
        "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
        "        # The components are an embedding layer, a 2 layer LSTM, and a feed-forward output layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.model = nn.ModuleDict({\n",
        "            'embeddings': nn.Embedding(vocab_size, embeddings_dim),\n",
        "            'lstm': nn.LSTM( \n",
        "                embeddings_dim,\n",
        "                lstm_dim,\n",
        "                num_layers=lstm_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_prob),\n",
        "            'ff': nn.Linear(lstm_dim, vocab_size),\n",
        "        })\n",
        "\n",
        "        # Initialize the weights of the model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_params = list(self.model['lstm'].named_parameters()) + \\\n",
        "                     list(self.model['ff'].named_parameters())\n",
        "        for n, p in all_params:\n",
        "            if 'weight' in n:\n",
        "                nn.init.xavier_normal_(p)\n",
        "            elif 'bias' in n:\n",
        "                nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, input_ids, input_lens, hidden_states):\n",
        "        \"\"\"\n",
        "        Defines how tensors flow through the model\n",
        "        :param input_ids: (b x sl) The IDs into the vocabulary of the input samples\n",
        "        :param hidden_states: (b x sl) x 2 Hidden states for the LSTM model\n",
        "        :return: (lstm output, updated hidden stated)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings (b x sl x edim)\n",
        "        embeds = self.model['embeddings'](input_ids)\n",
        "\n",
        "        # Pack padded: This is necessary for padded batches input to an RNN\n",
        "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
        "            embeds,\n",
        "            input_lens,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "      \n",
        "        # Pass the sequence through the BiLSTM\n",
        "        lstm_out, hidden_states = self.model['lstm'](lstm_in,hidden_states)\n",
        "\n",
        "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
        "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        \n",
        "        lstm_out = lstm_out.reshape(lstm_out.size(0)*lstm_out.size(1), lstm_out.size(2))\n",
        "        lstm_out = self.model['ff'](lstm_out)\n",
        "        return lstm_out, hidden_states\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
        "  \"\"\"\n",
        "  Evaluates the model on the given dataset\n",
        "  :param model: The model under evaluation\n",
        "  :param valid_dl: A `DataLoader` reading validation data\n",
        "  :return: The accuracy of the model on the dataset\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  loss_all = []\n",
        "  states = (torch.zeros(lstm_layers, batch_size, lstm_dim).to(device),\n",
        "              torch.zeros(lstm_layers, batch_size, lstm_dim).to(device))\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      targets = batch[1]\n",
        "      input_lens = batch[2]\n",
        "      states = detach(states)\n",
        "      logits, states = model(input_ids, input_lens, states)\n",
        "      loss = loss_fn(logits, targets.reshape(-1))\n",
        "\n",
        "      loss_all.append(loss.detach().cpu().numpy())\n",
        "\n",
        "  perplexity = np.exp(sum(loss_all) / (len(loss_all)))\n",
        "  return perplexity\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states]\n",
        "\n",
        "def train(\n",
        "    model: nn.Module, \n",
        "    train_dl: DataLoader, \n",
        "    valid_dl: DataLoader, \n",
        "    optimizer: torch.optim.Optimizer, \n",
        "    n_epochs: int, \n",
        "    device: torch.device\n",
        "):\n",
        "  \"\"\"\n",
        "  The main training loop which will optimize a given model on a given dataset\n",
        "  :param model: The model being optimized\n",
        "  :param train_dl: The training dataset\n",
        "  :param valid_dl: A validation dataset\n",
        "  :param optimizer: The optimizer used to update the model parameters\n",
        "  :param n_epochs: Number of epochs to train for\n",
        "  :param device: The device to train on\n",
        "  :return: (model, losses) The best model and the losses per iteration\n",
        "  \"\"\"\n",
        "\n",
        "  # Keep track of the loss and best accuracy\n",
        "  losses = []\n",
        "  best_perplexity = 300.0\n",
        "  # Set initial hidden and cell states\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # Iterate through epochs\n",
        "  for ep in range(n_epochs):\n",
        "    states = (torch.zeros(lstm_layers, batch_size, lstm_dim).to(device),\n",
        "              torch.zeros(lstm_layers, batch_size, lstm_dim).to(device))\n",
        " \n",
        "    loss_epoch = []\n",
        "\n",
        "    #Iterate through each batch in the dataloader\n",
        "    for batch in tqdm(train_dl):\n",
        "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on \n",
        "      # things like dropout and layer normalization\n",
        "      model.train()\n",
        "\n",
        "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
        "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
        "      # zero them out\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Place each tensor on the GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      targets = batch[1]\n",
        "      input_lens = batch[2]\n",
        "      # Pass the inputs through the model, get the current loss and logits\n",
        "      states = detach(states)\n",
        "      logits, states = model(input_ids, input_lens, states)\n",
        "      loss = loss_fn(logits, targets.reshape(-1))\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      loss_epoch.append(loss.item())\n",
        "      \n",
        "      # Calculate all of the gradients and weight updates for the model\n",
        "      loss.backward()\n",
        "\n",
        "      # Optional: clip gradients\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Finally, update the weights of the model\n",
        "      optimizer.step()\n",
        "      #gc.collect()\n",
        "\n",
        "    # Perform inline evaluation at the end of the epoch\n",
        "    perplexity = evaluate(model, valid_dl)\n",
        "    print(f'Validation perplexity: {perplexity}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
        "\n",
        "    # Keep track of the best model based on the accuracy\n",
        "    best_model = model.state_dict()\n",
        "    if perplexity < best_perplexity:\n",
        "      best_model = model.state_dict()\n",
        "      best_perplexity = perplexity\n",
        "\n",
        "  model.load_state_dict(best_model)\n",
        "  return model, losses\n",
        "\n",
        "def build_model(model):\n",
        "    # Create the optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train\n",
        "    model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), model_file_name)\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "def gen_text():\n",
        "    states = (torch.zeros(lstm_layers, 1, lstm_dim).to(device),\n",
        "              torch.zeros(lstm_layers, 1, lstm_dim).to(device))\n",
        "\n",
        "    sentence_start = \"Art Deco\"\n",
        "    unk_token_id = vocabulary.word2idx['<unk>']\n",
        "    new_token = None\n",
        "\n",
        "    while new_token != '<eos>' and len(sentence_start.split()) < 29:\n",
        "        tokens = tokenize(sentence_start)\n",
        "        token_idx = len(tokens)\n",
        "\n",
        "        tokens = tokens + ['<pad>'] * (seq_len - len(tokens))\n",
        "\n",
        "        token_ids = [vocabulary.word2idx.get(token,unk_token_id) for token in tokens]\n",
        "        batch = collate_batch_bilstm([token_ids])\n",
        "\n",
        "        logits, states = model(batch[0].to(device), batch[2] , states)\n",
        "        logits = logits.detach().cpu().numpy()[token_idx - 1]\n",
        "\n",
        "\n",
        "        new_token_id = np.argmax(logits)\n",
        "        new_token = vocabulary.idx2word[new_token_id]\n",
        "        sentence_start = sentence_start + ' ' + new_token\n",
        "        print('\\r'+sentence_start, end='')\n",
        "\n",
        "def get_sentence_perplexity(sentence, model, vocabulary, seq_len):\n",
        "  states = (torch.zeros(lstm_layers, 1, lstm_dim).to(device),\n",
        "              torch.zeros(lstm_layers, 1, lstm_dim).to(device))\n",
        "  unk_token_id = vocabulary.word2idx['<unk>']\n",
        "\n",
        "  tokens = sentence.split()\n",
        "  tokens = tokens[:seq_len]\n",
        "  tokens = tokens + ['<pad>'] * (seq_len - len(tokens))\n",
        "  token_ids = [vocabulary.word2idx.get(token,unk_token_id) for token in tokens]\n",
        "\n",
        "  batch = collate_batch_bilstm([token_ids])\n",
        "  loss_fn = torch.nn.CrossEntropyLoss()\n",
        "  logits, states = model(batch[0].to(device), batch[2], states)\n",
        "  loss = loss_fn(logits, batch[1].to(device).reshape(-1))\n",
        "  loss = loss.detach().cpu().numpy()\n",
        "  return np.exp(loss)\n",
        "\n",
        "arr_string = [\"\", \"\"]\n",
        "arr_languages = [\"eng\"] #, \"fin\", \"ara\", \"kor\"]\n",
        "for language in arr_languages: \n",
        "\n",
        "  # if = True, model is build.\n",
        "  # if = False the model is run\n",
        "  do_build_model = True\n",
        "\n",
        "  # chooese tokenizer\n",
        "  #tokenize = lab_tokenizer\n",
        "  #tokenize = word_tokenize\n",
        "  #tokenize = regex_tokenizer\n",
        "  tokenize = regex_tokenizer_lower\n",
        "\n",
        "\n",
        "  training_file = \"train-\"+language+\".txt\"\n",
        "  dev_file = \"dev-\"+language+\".txt\"\n",
        "  model_file_name = \"/content/drive/My Drive/NLP 2020W/models/model_\"+language+\"_1000_regexlower\"\n",
        "\n",
        "  data_all_train = get_data_2(training_file)\n",
        "\n",
        "\n",
        "  #train_size = read_data.get_size(training_file) # = 9211\n",
        "  # My GPU does not have enough vRAM to compute the full set\n",
        "  # So use nr records equal to size_total\n",
        "  size_total = 1000\n",
        "  # Sizes for train,test,valid parts of training set\n",
        "  data_train_slices = (int(0.6*size_total),int(0.2*size_total),int(0.2*size_total))\n",
        "\n",
        "  # Parameters for sequencing\n",
        "  seq_len = 35\n",
        "  # Batch_size has been lowered from 128\n",
        "  # If too high, it might result in cuda mem-errors\n",
        "  batch_size = 8\n",
        "\n",
        "  train_data,test_data,valid_data = get_data(data_all_train,data_train_slices)\n",
        "\n",
        "  vocabulary = build_vocab()\n",
        "\n",
        "  test_dataset = TxtDatasetReader(test_data, vocabulary, seq_len)\n",
        "\n",
        "  device = torch.device(\"cpu\")\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"has cuda\")\n",
        "      device = torch.device(\"cuda\")\n",
        "\n",
        "  # Define some hyperparameters\n",
        "  lr = 0.001\n",
        "  n_epochs = 5\n",
        "  lstm_dim = 1024\n",
        "  lstm_layers = 1\n",
        "  embeddings_dim = 128\n",
        "\n",
        "  val_dataset = TxtDatasetReader(valid_data, vocabulary, seq_len)\n",
        "  train_dataset = TxtDatasetReader(train_data, vocabulary, seq_len)\n",
        "\n",
        "  valid_dl = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True , collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  model = LSTMNetwork(\n",
        "      embeddings_dim=embeddings_dim,\n",
        "      vocab_size = len(vocabulary.word2idx),\n",
        "      lstm_dim=lstm_dim, \n",
        "      dropout_prob=0.1, \n",
        "      n_words=seq_len, \n",
        "      lstm_layers=lstm_layers\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "  if do_build_model:    \n",
        "      build_model(model)\n",
        "\n",
        "  # Use model\n",
        "  model.load_state_dict(torch.load(model_file_name))\n",
        "  test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "  print(evaluate(model, test_dl))\n",
        "\n",
        "\n",
        "  gen_text()\n",
        "  if language == \"eng\":\n",
        "    arr_string[0] = 'I want to buy some potatoes from the airport.'\n",
        "    arr_string[1] = 'gibberish ? . something something is'\n",
        "  #if language == \"fin\":\n",
        "  #  arr_string[0] = 'Haluan ostaa perunoita lentokentältä.'\n",
        "  #  arr_string[1] = 'pilkkaa? . jotain jotain on'\n",
        "  #if language == \"ara\":\n",
        "  #  arr_string[0] = 'أريد شراء بعض البطاطس من المطار.'\n",
        "  #  arr_string[1] = 'ثرثرة؟ . شيء ما'\n",
        "  #if language == \"kor\":\n",
        "  #  arr_string[0] = '공항에서 감자를 좀 사고 싶어요.'\n",
        "  #  arr_string[1] = '횡설수설? . 뭔가 뭔가'\n",
        "    \n",
        "  print20 = get_sentence_perplexity(arr_string[0], model, vocabulary, seq_len)\n",
        "  print21 = get_sentence_perplexity(arr_string[1], model, vocabulary, seq_len)\n",
        "  print(print20)\n",
        "  print(print21)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132266\n",
            "has cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "100%|██████████| 8680/8680 [22:37<00:00,  6.39it/s]\n",
            "Evaluation: 100%|██████████| 2871/2871 [02:22<00:00, 20.18it/s]\n",
            "  0%|          | 0/8680 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation perplexity: 1016.560258891359, train loss: 6.9594179947255395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8680/8680 [22:48<00:00,  6.34it/s]\n",
            "Evaluation: 100%|██████████| 2871/2871 [02:22<00:00, 20.15it/s]\n",
            "  0%|          | 0/8680 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation perplexity: 1027.1561573139684, train loss: 5.883479371433434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8680/8680 [23:29<00:00,  6.16it/s]\n",
            "Evaluation: 100%|██████████| 2871/2871 [02:22<00:00, 20.11it/s]\n",
            "  0%|          | 0/8680 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation perplexity: 1201.2752602532607, train loss: 5.092431089130964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8680/8680 [23:30<00:00,  6.15it/s]\n",
            "Evaluation: 100%|██████████| 2871/2871 [02:22<00:00, 20.10it/s]\n",
            "  0%|          | 0/8680 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation perplexity: 1463.3214460610277, train loss: 4.514280175997914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8680/8680 [23:31<00:00,  6.15it/s]\n",
            "Evaluation: 100%|██████████| 2871/2871 [02:22<00:00, 20.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation perplexity: 1760.3601384418732, train loss: 4.132313304095773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfoH8O/JpEGAQAq9BAIhFKmh9yYIKK6FxRV/ujbsKLYgdl1EdHftKJZVURCwoTTp0sGE3gIJBEIJCQRISEid8/tj7vSZzEym3uT7eR4e75xb5p1r8ubMuacIKSWIiEh9gvwdABERVQ0TOBGRSjGBExGpFBM4EZFKMYETEalUsC/fLCYmRsbFxfnyLYmIVC81NfWClDLWstynCTwuLg4pKSm+fEsiItUTQpy0Vc4mFCIilXKYwIUQXwkhcoQQB0zK3hFCHBFC7BNC/CKEqO/dMImIyJIzNfCvAYyxKFsNoLOUsguAowCmezguIiJywGECl1JuBJBnUbZKSlmuvNwOoLkXYiMiokp4og38XgAr7O0UQjwohEgRQqTk5uZ64O2IiAhwM4ELIWYAKAfwvb1jpJRzpZRJUsqk2FirXjBERFRFVe5GKIS4B8B4ACMkpzQkIvK5KtXAhRBjADwH4CYpZZFnQ7K29vB5zNmQ4e23ISJSFWe6ES4AsA1AeyHEaSHEfQA+AlAXwGohxB4hxKfeDHJDWi7mbmQCJyIy5bAJRUp5h43iL70Qi12aIIFyLVtpiIhMqWIkphD+joCIKPCoIoETEZE1JnAiIpViAiciUikmcCIilWICJyJSKfUkcPYiJCIyo4oELsB+hEREllSRwImIyBoTOBGRSjGBExGplCoSeIVWi4KScscHEhHVIKpI4N9sOwkA0HJCKyIiA1UkcD2mbyIiI3UlcC78Q0RkoKoETkRERkzgREQqpaoEzgYUIiIjVSVwIiIyYgInIlIpJnAiIpViAiciUilVJXB2AyciMlJVAiciIiNVJXDJjoRERAaqSuBERGSkqgTONnAiIiNVJXAiIjJiAiciUikmcCIilWICJyJSKVUlcD7EJCIyUlUCJyIiIyZwIiKVcpjAhRBfCSFyhBAHTMqihBCrhRDHlP828G6YOhyJSURk5EwN/GsAYyzKkgGslVK2A7BWeU1ERD7kMIFLKTcCyLMongDgG2X7GwA3ezguO7H44l2IiNShqm3gjaSU55TtbACN7B0ohHhQCJEihEjJzc2t4tsREZEltx9iSiklKllvWEo5V0qZJKVMio2Ndeu9CorL3TqfiKg6qWoCPy+EaAIAyn9zPBeSfTkFxb54GyIiVahqAv8NwN3K9t0AlngmHCIicpYz3QgXANgGoL0Q4rQQ4j4AswCMEkIcAzBSee11fIhJRGQU7OgAKeUddnaN8HAsRETkAlWNxGQFnIjISF0JnG0oREQGqkrgRERkpKoEzvo3EZGRqhI4EREZqSqBhwdr/B0CEVHAUFUCj6kb6u8QiIgChqoSOBvBiYiM1JXAiYjIgAmciEilVJXA2YJCRGSkqgRORERGqkrgM3454PggIqIaQlUJfM3h8/4OgYgoYKgqgRMRkRETOBGRSjGBExGpFBM4EZFKqS6B/3Ew298hEBEFBNUl8CnzUjFv+0l/h0FE5HeqS+AA8OWm4/4OgYjI71SZwDMvFvk7BCIiv1NlAjd14kKhv0MgIvILVSfwP4/mYti7G/Dr7jP+DoWIyOdUm8CllHjpV93cKL/ucT2B/5R6GsfOF3g6LCIinwn2dwBV9d32kziVp2sLz6xCM8rTi/fqzp01zqNxERH5impr4H9lXjJsCyH8GAkRkX+oNoH/tvesYVtKiYtXS7DoryzM+GU/yiu0foyMiMg3VNuEYunOL3bgSLauTXtkx0YY1r6hnyMiIvIu1dbALaXnXPV3CEREPlVtErjpepm7T15CXPIyZOVVfcDPgTNXsOlYrvuBObA36zJe/HU/pOSKn0TkmmqRwIUQZgnwg3XpAIBFKVl45PtUFJdVOHWdTcdycTxXV5Mf/+Fm3PXlTs8Ha+GOz7fju+2nUFTqXIxERHqqaAOvGxaMgpJyu/vtjcb8UEnkN3XNxZjOjW0eM+GjzegVF4WbuzczJGx2LSQiNVBFDfzhYfEevd76tBzD9t7TV/DF5hMY/+Fmj76Hs9hyQkRVpYoEHhascev8h75LxWPzd+Gn1NM4c/ka/vm/v1w6v7iswmbXxLjkZXhlyQG3YtNjV3YicpVbCVwI8ZQQ4qAQ4oAQYoEQItxTgXna0n3n8PTivXj4u1SXz018aaXd9vBvtrk3N7lUHr8ePpfv1nWIqOapcgIXQjQD8ASAJCllZwAaAJM8FZi3ZFSxu+G24xft7vt+x0nEJS/DEwt2u3xdfRPKrXO2VSkuIqq53G1CCQZQSwgRDKA2gLMOjq+SeuGee9Za6IXeHq//fgiAbnSoVutao7bp0Uv3nUVc8rIqze1CRDVPlRO4lPIMgHcBnAJwDsAVKeUqy+OEEA8KIVKEECm5ud7vV+1NpeWOh+i/uyrN6etdvFpids2le88BAH7ZfQZxyctwuajU9SCJqMZwpwmlAYAJAFoDaAogQggx2fI4KeVcKWWSlDIpNja26pH6SUFxmWE74cUVOHP5WqXHrzp03ulr93xzjdlrfXv4+2uPAQBeU2r2RES2uNOEMhLACSllrpSyDMDPAPp7Jixz/pxt8MVfzXuZLNx5qtLjK7QSJy8WorCk3OUatGWXwpJyDu4hIvvcSeCnAPQVQtQWugw7AsBhz4RlbkDbaG9c1ilL9pg36+tHeZom1xKTZpATFwox5J0N6PTKH+j2+mpsPOp8sxG7hBORK9xpA98B4EcAuwDsV64110NxmWkSWcsbl62yvVmX0f7FlU4du/vUZZy6WISVB3Tt27NXHsEXm447dS4H+RBRZdzq3iGlfAXAKx6KRTUmfLzF6WMrtFoMfmc9AN0Q/U82ZAAASm0MDDqSbd4XnAmciCqjipGYvjZvW6bHrqVvcgFg1sVw9krr3ipZeZU/ICUiMsUEbsNLSw565bqfO9l0oqdlFZyIKsEE7kMHzro2XN6VLolEVPMwgRMRqRQTuA+tO8waNRF5DhO4D3ljHhZPulRYiud/3Of0CkZE5F9M4GTw79VpWJiShcWpp/0dChE5gQmcDNjphUhdmMCJiFSKCZy8QkqJiZ9uwxp2hSTyGibwGuxIdj4KS8q9cu2Sci12Zubh0fm7vHJ9IlJRAp/Qram/Q6hWSsu1GPPeJjzkYI3QguIybD52weXrH1QGLbFZnch7VJPAA21GwkBVXqHFrlOXHB6nH6a/80QeAGBbxkWbKw5N/WEPJn+5Azn5xS7FceucrboNZnAir1FNAp82KsHfIajCf1YfxS2fbMX+01ecPudIdj7u+Hy7sfuglNBqJUrLtUjLLgAAZF2q2kRb0iKD5+QXo8zGTIxE5DrVJPDQYNWE6leHz+maLnIKnK8xn7xYZFU2/ef9ZkvIzfhlv2HfqoPZWLLnjFPXNu2aWFhSjt4z1+LlJQfsn0BETlNVVnzlxo7+DiFgLd9/Dkey86u0/NyUedbt4AtTssxel1Voca20AnHJy/DgvFRM/WGP1Tnbj1+0+sOhz9/XSitQWKp7YLqaPVOIPEJVCfyO3i39HULAeuT7XRjz3iavvsclB2t8Tpq7Hbd8stWqfODb69Dh5ZWYoyxmISWQfaUYD3+Xij1Zl1FQXIYKLRvLiVzl1oo8FHjWHckB4HhU5VWl+6CzaTOvsNSpY09fuoaDZ43t7xVaidNK+/n/tmQayu/6cgeO5VzFigPZAICmkeHYOn2Ek9EQEaCyGjgB+05fxicb0h0eJ6Fr9kg9mWe1r7xCi6Q317j0vpeKyjBrxRG7+zMvFBq2x32w2eH1cq+WmL0+e6UYKZnWsRKRfUzgKnPTR1tsLsdmy9srjuDWOdvMasQAkHa+wLBtq+tgiY0yAPh971m77/X+2mNOxaR3uajMquySjTIiso8JPMCl5xTg512nEZe8DHmFlbdBmzqeexVHlC6AF66WIi55Gb7afAJXS8od1pA3OTlwJy55maHt2vKPRGUu2vkcD3ybgq3prg8aIqqpVJXAQzVBiIuubXjdOy7Kj9H4xsj/bMQ3204CAE5eLLTaf6mwFCXl1vN3v2XS3FGh1dWoZy4/jKvFjofO/3k01+n44l9YjqHvrMfR81edPqcy765y7tsFEansIWZQkMCGZ4fh3JVruFxUhvScq9hZA9pNS5QFFiwfIi7bd67SuUY2K7VZ0weaX2/N9HB0QKaNfuRVxb4oRM5TVQ1cr0lkLXRoUg/juzTxdyg+oW8KMU3EOfnF2JzuXE1Z30OvXCvx6Z8Zng7PozgnOZHzVFUDt1SVQStqpjXJbr1nrnX6vMdUNCMg8zeR81RZA6+pnrQx+tEZ9nqVBCRWwYmcxgSuIvp5SaqzvS5MwkVU0zGBExGpFBM4EZFKMYETEamU6hP4s6Pb+zsEIiK/UH0Cf3RYWyTfkOjvMIiIfE71CRxgzzPyn6LScmw/frHSY65cK8MxkwnEiDyleiRwDv8gP3l28T5MmrsdZy5fQ3qO+XwweYWlkFLilk+2YNR/N/opQqrOVD0SU89eDfzVGzvi1d8P+TYYqpbSsgsQHxuBYI15nUe/BulH69KxYOcp/ProAHRrUR9HsvOtVkjamnEBTyzYjVqhGozs0AgD4mPQICIEPVtV/0nZyDvcqoELIeoLIX4UQhwRQhwWQvTzVGDuuKd/HB4c3AZ39m3l8Njv7+/jg4hILR75PhU3fWQ+3W7mhUKMfm8j3l5pvaCFvu6wJ+syACArTzex16K/Tlsdm55zFReuliIr7xr+tyUT93+bglvnbPPsB6Aaxd0mlPcBrJRSJgLoCuCw+yG5TmuxnmL92iF4YWwHhGgcf7z+8dG4f2Brb4VGKrN8fzb2WYwGvaCsHrTr1GWr46Xy9c90Vp7yCi2+2nLC6tiXlxz0XKBEcCOBCyEiAQwG8CUASClLpZTWP+E+9M8BcZjctyUeGNTG7jFv3tzZ7LUQAi+O52r3ZO7o+QLEJS/DbxarEH249hgem78Lw97dgAqtNEylq5+r/fEFu3H9e661d18rtZ7PncgZ7tTAWwPIBfA/IcRuIcQXQogIy4OEEA8KIVKEECm5uc4vFOCKuBjd217XLBJv3nwdIsLsN+1PdqJZheinVF0TyC+7zJtC/r36KJbuO4cTFwoR/8JyQ3mhSRI+nmu98EZl3vmDi1hQ1biTwIMB9AAwR0rZHUAhgGTLg6SUc6WUSVLKpNjYWDfezr7xXZrgl0f642/dm9ncfxeTNpk4dr4AH6+vfGHozzYeB2A+va2t9UM9oaCYa4FS1biTwE8DOC2l3KG8/hG6hO5zQgh0b9nA5vzgmbPG4Q2LZpNv7u1t91pjOjX2eHwUWG6dsxXv/JGGYmWlo83HLuDnXdYPHQHg1MUi/JV5CQCw/4x3Zko8cDbfK9el6q/KCVxKmQ0gSwihH8s+AoAq+uwNSYhF08hwm/v+9bfONst3vTQKXZpH2r3moHYxHomNnHfhagkycl1fi7PYoiY9+csdmLZoL/q9Zb1IxvELhTZ7n3iSvisikavc7YXyOIDvhRD7AHQDMNP9kHzj18cGYL6NLoT22s+jIkJxU9emNvf9PakF5t3H7oi+Nnj2eoz495+VHnOlqAyLUrLwy+7T0GoljmTnG3qMrD2cg7jkZYZjz10p9mK0RJ7n1kAeKeUeAEkeisWrdrwwAleuGdsaG9YNR8O61rVw00FB/dpEY5vJMOlecbYHXOhbboYkxLq0ojs5b962TMzfmYUVUwcZyooc9N4oq9Ci6+urDK+P5xbiw3XGtu+VB7M9HieRL1WLofTOaFQvHAmN6rp0Tr1awRavQwzbU0e0M2w3rBsGAPj6n71w4q2xbkRJ9ry05CAOn8vHHwez8czivWb7Fuw8hXf/SMPXSt/rvVmXkfDiCvyYat6ubZq8AeB3iy6CRGpTYxK4s8KCjbfkoSHxAIDExrrErx+0ERddG0+NSsDB10bjPxO74rHhumQuhDB7kBpTJxTv3Nal0vc7PpMJ35Zdpy7hfL51k8aUealWiXn6z/vx0fp0w7QJr/1+EKXlWkz/eb9PYiXyFyZwC0FBApmzxiFz1jjDSM4gJSnrW1f0SToiLBi39GiO0GDbt/HZ0e1xe1ILh+9H1m75ZCtGVtK+bSu5A8CcDRmo0HJys0BVVFpuqAiR+5jAXaD/uXOUcltE1QIA9G0TbbVvcIJ3+sJXJ/pf8IKScuQVlto8ps9M6x4jAPD2yiNcGDkAXS4qRVzyMnR8+Q98scl6mgGqGibwShgStpKxQzS6jaiI0ErPE5Wk+Pf/3s2w/cODfd0LsAZ48NsUdDd5EEnq9OVmY9L+13K/TJlULTGBV0I/z7g+gbeKjsCbN3fGnMk9nTvfwTdFWzV0AHhgUM2eXKv1dOMQ9ZSTl3CpiCMV/ens5WtWo1Djkpdh2sI9Vscu+isLi1KyDM1YxWUV+ONgttUDZPKMajEfuLeZ1qidmUslpk4oTuUVQeNi+/b7k7ohpk4Y+sdH43N+zSQ/Ky6rwJwNGXh/7THc2qM5/j2xq9n+n3efwR19WuKvzDw8MrQtvtx8Am8s1T1IzsorwtPXt0fiSyvtXjs8ROP1z1DdMYFXoqrPWj67KwlrDp9Hi6jaVvtClAeetWz88E7oZnsuFyJ/+HDdMXy8PgMAsCEtx+Yxt3+qm8/80Nl8LN13zlC+/8yVSpea0/JBpkcwgVciUun3re9G6KzYumG4o3dLq/KfHu6POmHBWPr4QEPfcQDYOWMEtN6ZJ4nIoLRci7kbM3D/oDZWtd+vt5xA2vkCJI/pgK6vr0KtEA0mJjU37L9YWGoYtfqEyRgIPdPkDQAb0nKxIY2D2ryNCbwScTERWPxQP1zXzP4cKK7o2aoBAKCzxfVsjQgl8qSc/GK8uyoNi1J0fej1Yxf09H3oVx/S1bSvlVXgm20nbV7rg7XHvBhp9fHl5hPoHReF9o3rYk/WZfRu7fml8/gQ04FecVF+aatrFW3d/ELkiJQSo/+7EUv3mY8y7T9rnSF5v7vqKPZm2V57Rb/6kLddLSn3yft4y8+7TiMueRmKSu1/jjeWHsKNH23GzOWHMfGzbV6ZtIwJnKiakFJi3+krSDtfgGkLjdMNFBSXodxicNOEj7fg8Ll8zNmQgf1+6Df/gspHyb63RvctJCff9h880/ELR7J1iftSke0xDe5gE4oP3DewNR/akFedvlSEse9vQn6xeY2wuKwC320/ZfOcG97fBAB42+vRWVtz2PZDUW+7VlqBotJyRNcJc3ywE4QArlwrQ0FxGZo30H1r3pZxEXd8vt1wzPbjeR55L1tYA/eBl8Z3xCs3dnLpHA6wp8r8vvcs4pKXoaxC9/R74NvrzZK3hMSvu88g8aWVVZozvbr62ydb0PPNNS6fV6GV+L+vdmKH0rNGP0Zk07EL6PraKgx8ez1ST+oStWny9jbWwBUrnxyEYM5LQgFu+s/78VPqaZQqibvdjBXInDXO6riyCoknlYE2lpN/1WRHsgtcOn5P1mVcKizFrlOXsPFoLtKy87Hggb6GhahXHDD2vknJvISerew/qLzshQFpTOCKxMb1/B2Cmcl9W+HNZa4POZ42KgERYcFo0aAWOjWLxIBZ6zwW01f3JOHer1OcOrZxvXBk25lwilyTkXsVC3acwoxxHbBgp+3mELLv+R/3YWFKllNTPecXl6FOqC4tXi0tx80fbzHbfz6/BMNNJlnbkm7s6/7WiiPo0ry+3WsXeuHBLRN4gLp/UBuUayVmrXBtOa9gjcB9Ax0PxV/11GBopcSY9zbZ3D9tVAI+XHcMZRXGtvvhiY2sjmsRVQtZedfMyg69Phq3fLKVCdwFWq3E/jNX0LWFdQK47+u/kHmxCB2b2q5k6Nf2JNsWpmQBAL7bbrtbpF5BcRm6vLoK3VrUxx47vXQcqaz5xBtPwdgGHsD+0cd6MNAjQ+MrPcfZZ6UJjepW+q3jiRHt8NtjAw2vba35+dTIBDw3OtGsLEQjUDvUtXrBpueGuXR8dfT5puOY8PEWs9GL3+84iaPnC5B5sQgAMG3RXpvn2huuXlNduFpic0rhdUeMD06XKQOPpJRYsucMSsorsOuULmlXNXk75IUMzhp4AKsXHmJV1rVFffz+2ECknswzDL5wxfwH+jjdr71Dk3pIviERUbVDMaG7+Xqgt3Rvhqkj22HNofNm5X/vpZv/vH3jujiSXYCNzw5DUVk5lu/Pxgdrj+HGrk2tVsKxNeVATaNfmf7sZd23mVd/O4ivt2b6MSJ1yissRdKbazBlcBtMH9vBbJ9p/nx0/i5cLbkOjeqFY+oP1pNyeYP0QgZnDVxlmkbWwnXNI9GwnvnozfjYCKfO7x8fgx4tGzj9fg8NicfEXi0QFmye9OvX1k2pazphV92wYLx+U2cAwKxbumDBA33RMro2EhvXM6x01KJBLZvvc++Amj0Do/6Pmv4bFJO3a7RaidOXipBXqOuXvebweetjLPLny0sOYvn+c1bHeYs3ehIzgQe4AW2jMblvSxyfORYrpg7Cdc11w/D1PwwjEhsiY+ZYjO7U2CfxvHpjRwC6tnYAaFLf+IckLERjWGGoVqgG/eKtp8u19zN8s0UNv6bKzi/G4Nnr/R2GKvxxMBuT5uom0/pofToGvr0eGbmFAIyraOm7WQJA/jXzXiAl5VrD6FRf8MZCUWxCCXDf329c9KFDE2Obtf7rWHiIBpoggQFtY/DJhgz0ivP8fAumJvVuiaxL1wwTGoVqTOsAzv2E/vnsUPx5NBcvLzloKKvs6X1N8s4faf4OQTWmzEsFoGvH3px+AQAwe6Xuof+xHF3f93YzVhiO91rbtpO80YTCBK5Shq9jSgvGgLYxOPLGGK/P2xIeosFL4zsaXpsu4lyZu/q1wuFz+ZgyuA3q1w5Fg9pc9oycl5VXhKAggb9O5GFguxjE1AmDELrfAymBK0ofa30NPBB5owmFCVylbP0seCJ5P3N9Aq650C2tuUmb9jf39rZ7XL3wEHz0jx5uxUbVm1YrsepQNq7v2Nhqse9BFs1Kr0/oZEiIbV5YDlv0098Git/2nHVqQRhXMIGrlH7UqP7hoCN9Wkfhn048KLScZtSREJMmlE5NPTPtLtUMUkqzb3CLU7Pw/E/78cbNnXGXg0Rn2vymFjszPT8nCh9iqtT1HRvh4aHxeGV85XOsNFZ6qyyc0g9jOvvmQSeRMywHqZ1XZvbLsRgA9sqSAz6LSW1YA1epYE0Qnh+T6PC4FVMHOTXH87bpw1FSxmWByHe+3XbSqq+23m97z+KJBbt9HJH6sAZezTWICEW7Ro6XhGsSWQtxMc71JfeE4YkNMahdDP58dqjP3pMCy7WyCsQlL8POE3koLCk3e8jH5O0c1sDJbb1bRxlGEDorIiwY8+7r46WIAs/7k7r5bMSf2kz8bJvZ6w/XpfspEvVhAie3LZrSz98heFWbmAgcv+Be97SBba3nkiFyF5tQargv/i8J79zWxd9hBKzbejbHTw/3t7kvOkI3nUBCozoOrxNdJwyZs8Zhqo0V3Ymqigm8hhvZsRFuT2rh7zAC1ru3d0UDJVHrPTBI1x0zSikPDnL+12hwAmvi5DlM4BQwQjSBuyLSwddGG7b1fZf1XZidHIwKAKgTZj3DJFFVMYFTwPhrxkh/h2BXRJjxcdHQhFgAuoe3ABDq5GAqQDdr5LguTdDNxsINRK5yO4ELITRCiN1CiKWeCIhqLv0Utd7yxIh2WPr4QLMyWyNZHY0C7N82Bun/ugHdW+im5Y2LjsDsW62fI+hnbmwVbZzvPFgThI//0cNsYjKqGTReWHPXEzXwqQBcX7yRyAc2P29c7efx4W3RuZlxuP89/eOsEjoA3NKjmcPrBmuCDFPqBgcJTOxl/hyhYd0w3DOgNTJnjcOfz1qvOORKswtVD+Oua+Lxa7rVjVAI0RzAOAD/AjDNIxERmRjWPhbr03IBAK1jInCiit35mtWvZTZvCwC8epPtaQi621nwYsqQNsgtMI5qHXtdE+w/fQWPDW9rdlzGTMeL53ZkDbzG6dLc83MFuVsDfw/AcwA4Bpu8oqXJcmvrnxla6bFPjtR10XO1eWJAW+uFJ2yZfkMH/GdiN8PrEE0QXhzf0azpJzoiFJog4fDr8p19WmL5E4NcipPIUpUTuBBiPIAcKWWqg+MeFEKkCCFScnNzq/p2VEOceMu89vrCONtzZdjy5MgEZMwci2U2mkUqM/euJPzx5GCXzrFlwQN9sXyqc0lZCGF3lXkiZ7lTAx8A4CYhRCaAHwAMF0J8Z3mQlHKulDJJSpkUGxvrxttRTSCEQPq/bjC8DgvWQAjjmpnv3t610vM1QcJqLmlHIsKC0b6x4/liHOkXH41GFmuV1iTN6tte75S8p8pt4FLK6QCmA4AQYiiAZ6SUkz0UF9VgwZogdG9ZH12VZdZOvDXOsO+2ns3xzOK9Tl8rtm4YIkI1mD7W8cyNppY+PhCRtdhn2xVrnx6C8BCNzYUUoiNCcbGw1A9RBY6mXvgDx7lQKCD98sgAj1wnLFiDg6+PcerYGWM74PI1XZIx7a1COkfeGIPEl1ba3BccJCpdEUrfV/7/+rXCt9tOeiW+QJfogW95ljwykEdKuUFKOd4T1yLytt52Fn5+YHAbPDvatZp6dTWsve3mzifszOXiaGWoBGVK40m9WroXmIq19sJ0zRyJSaqVOWuc44NsWDilr9XDUjLXt42xZ06o0v1SCGBkh4ZVut5H/+iO7+/vg5g63h2sFcicXQDcFUzgpErufB0VQnjll0ltJnRrava6S/NIw3w0/+ijqyk/Oize4pj6yJw1DjteGGFW/vwNxm8uE5OaW71X3fAQDGgbA/C2exTbwEl1Vj45CE0idQ+E0t4cg/YvWrfLdmkeiX2nr/g6tIDVJDIc566YrzX5+k2d0SamDh4f3ha/7zuL/vExGDBrHQCJEE2Q4RtO43rheOW3gwgxmXXRtLeN5Tchy9kbyXtYAyfVSWxcz2hhRKcAAAm7SURBVNBDJCzY9oOz7+7vgxVO9smubib1sp4euH+89TS2kbVDMHVkOwQFCUzo1gyxdcMMU+SafkG5q18cjr81zqp7ZnxshGG+F1PCopq9c8YIu/uqo+fGtPfZe7EGTtVSvfAQ1GtSM7sBDm3fEKsPncfFwlL8/thA1AkPRtP64Th35Rq2ZlzELd2b4alRCTbPXfxQP2xJv2D3D6OptU8PtVmuT/5jr2uMV2/shIZ1jbV1L8zn5DXXd2yEVYfOu3zeQ4PjMXtlmhcissYaOFE1cXtPXdvzmM6NDV366tcOQeuYCIQFawzTEvRqHYUWJlMUmGoRVRuTervXU0Sfozs1jURDi4FNQSp69vDuxMoHjQFA1xb18c29vc16Nrk6kMwdTOBE1cTs27oYeteM6dwYgO6biN5YZTa8pFa2J+vyNGm6zLwiEPN3sJ2Ea3rv7BEAhiTEYtFD/TCyQ0PD8nqZs8aZPRv46p4kj8RqiQmcyI/6x5tPpOVocYjKEoFp75oXxnZA6osjEVnbmIQGJ8Qic9Y4tGvk+QElpjo11Q2Cat/Yeq6XQOn9YzrZ2OKHrBfl/rfFlA36BTjm39/HZjkAfHF3L6x6aojN9xue2KjKsVaGCZzIj+Y/0BdtYnUDPO7u1wppbxhHjWbMHIvxXcznkHY2EWiCBKLrhHkuUBeM69IE654eglEdrWMNhPx96PXROGJyn239Qbu1p3lXyIVT+mLNtCHo39b8YfAMB5Ot/fRwP/z3746bYqqKCZxUb/0zQ7Fmmu2ajxp0UGqqvVpHmdVQNUHCZpvxQJMkMufOHt4PsAraxNaxWR4IbeBhwRqEaIKw66VR2D59BOqEBeP3xyqfwTIsWIO2Da0/k+Uc85Z6torC37pb94v3FCZwUr3WMRE2f7nUyt7izvr21e/u74PhiQ2VY9X1K+yt53umKy85KyoiFI0jdQ9ZE5vUxdjrGmP2bV3Muj3a0lVZmKGqI4E9id0Iifzs+k6NsGz/OcNCFBufG4bz+SVmx7w8viPuMOkdos+D1o8JA5u3auDNG1j3qukfH40XxnbA+A83Ozw/RBOET+7saVX+22MDcLWk3Kxs3v19kJVXVPVgPYgJnMjPJnRrhhs6NzE8wGwSWcsw0nR4YkP8tvcs+reNRq1QY99s0zw4/4E+OHkxMBKKt0zu2xJN69fC7JVp+Fv3Zph+QyL6zVqHCq31n7DP7uqJhnXDrJbG+3RyD3y28bhL3wK6NK9vVVYvPMTwoNbfmMCJAoC93ic3d2+GUR0bISLM9q+qlBL942PQP97m7oBT1Rr4C2M7YOnecwCAiDANGtYLx8bnhuHx+bsMzUl6ozs1tnmNMZ2bYExnzy8s7E9M4EQBzlby1jcZ1FPZohP22vcdERC4uXszHD1fgMeVKW2b1a+Fnz00b7xaMYETqVDyDYno3TrKbNpXNXClH/iQhFhsTr9gaCYJDdYtIm3Pp5N7oKC43Kp89q1doLUxqKg6YAInUqHwEI1hZGV19dU9vbA4JQuv/X7I4QAnAHabRybamNyrumACJyK/+/be3mhaPxwN64Wjy6urUCcsGJoggUm9W7o9N0t1pq5OpESkeq9P6GRVNjghFm0b1jV0j3S0RBvp8C4RkU/d2aeV3X11w0MwbVQCFk7p68OI1ItNKEQUUOwtnEzWWAMnIp/SWIykGdTOerUgcg4TOBH53N5Xrke98GCse3oI5t3Xx/EJZBObUIjI5yJrhWDfq6P9HYbqsQZORKRSTOBERCrFBE5EpFJM4EREKsUETkSkUkzgREQqxQRORKRSTOBERColpA8nOhdC5AI4WcXTYwBc8GA41QXvi228L/bx3tgWyPellZQy1rLQpwncHUKIFCllkr/jCDS8L7bxvtjHe2ObGu8Lm1CIiFSKCZyISKXUlMDn+juAAMX7Yhvvi328N7ap7r6opg2ciIjMqakGTkREJpjAiYhUShUJXAgxRgiRJoRIF0Ik+zsebxBCfCWEyBFCHDApixJCrBZCHFP+20ApF0KID5T7sU8I0cPknLuV448JIe42Ke8phNivnPOBEMJ8XasAJIRoIYRYL4Q4JIQ4KISYqpTX6PsCAEKIcCHETiHEXuXevKaUtxZC7FA+z0IhRKhSHqa8Tlf2x5lca7pSniaEGG1SrtrfOyGERgixWwixVHldPe+LlDKg/wHQAMgA0AZAKIC9ADr6Oy4vfM7BAHoAOGBSNhtAsrKdDOBtZXssgBUABIC+AHYo5VEAjiv/baBsN1D27VSOFcq5N/j7MztxT5oA6KFs1wVwFEDHmn5flLgFgDrKdgiAHcrnWARgklL+KYCHle1HAHyqbE8CsFDZ7qj8ToUBaK38rmnU/nsHYBqA+QCWKq+r5X1RQw28N4B0KeVxKWUpgB8ATPBzTB4npdwIIM+ieAKAb5TtbwDcbFL+rdTZDqC+EKIJgNEAVksp86SUlwCsBjBG2VdPSrld6n46vzW5VsCSUp6TUu5StgsAHAbQDDX8vgCA8hmvKi9DlH8SwHAAPyrllvdGf89+BDBC+bYxAcAPUsoSKeUJAOnQ/c6p9vdOCNEcwDgAXyivBarpfVFDAm8GIMvk9WmlrCZoJKU8p2xnA2ikbNu7J5WVn7ZRrhrKV9vu0NU0eV9gaCbYAyAHuj9KGQAuSynLlUNMP4/hHij7rwCIhuv3TA3eA/AcAK3yOhrV9L6oIYETdDUu6GpYNY4Qog6AnwA8KaXMN91Xk++LlLJCStkNQHPoaoaJfg7J74QQ4wHkSClT/R2LL6ghgZ8B0MLkdXOlrCY4r3zNh/LfHKXc3j2prLy5jfKAJ4QIgS55fy+l/FkprvH3xZSU8jKA9QD6QddsFKzsMv08hnug7I8EcBGu37NANwDATUKITOiaN4YDeB/V9b74+2GDo38AgqF76NQaxocGnfwdl5c+axzMH2K+A/OHdbOV7XEwf1i3UymPAnACugd1DZTtKGWf5cO6sf7+vE7cDwFdu/R7FuU1+r4occcCqK9s1wKwCcB4AIth/rDuEWX7UZg/rFukbHeC+cO649A9qFP97x2AoTA+xKyW98XvN9nJ/xFjoeuBkAFghr/j8dJnXADgHIAy6NrV7oOuLW4tgGMA1pgkHQHgY+V+7AeQZHKde6F74JIO4J8m5UkADijnfARlFG4g/wMwELrmkX0A9ij/xtb0+6LE3QXAbuXeHADwslLeBro/SulK0gpTysOV1+nK/jYm15qhfP40mPTCUfvvnUUCr5b3hUPpiYhUSg1t4EREZAMTOBGRSjGBExGpFBM4EZFKMYETEakUEzgRkUoxgRMRqdT/A3YgdLwzziyTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 2765/2765 [02:17<00:00, 20.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1968.9958841573948\n",
            "Art Deco architecture in the late 1930s and early 1960s the early 1960s and 1970s began in the 1930s with the increasing number of years before the age of10.963915\n",
            "11.408741\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}