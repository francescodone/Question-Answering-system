{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aatiVcD_GFp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX4RsxBn_Gr4"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def enforce_reproducibility(seed=42):\n",
        "  # Sets seed manually for both CPU and CUDA\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # For atomic operations there is currently no simple way to enforce \n",
        "  # determinism, as the order of parallel operations is not known.\n",
        "  # CUDNN\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "  # System based\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGBE9f0k_q9D"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2BreGgANvB"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irn2X-jZBXph"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5bntTo-JhG"
      },
      "source": [
        "#a4_data2label\n",
        "\n",
        "import json\n",
        "from langdetect import detect\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "\n",
        "# Task 4.1\n",
        "\n",
        "# We want to convert the JSON-files to a format of tokens (words) along an IOB tag\n",
        "# This tag corresponds to whether the given token is\n",
        "#  - the beginning (B) of the answer\n",
        "#  - inside (I) the answer\n",
        "#  - outise (O) the answer\n",
        "#\n",
        "#\n",
        "# We store the resulting tagged words in a file.\n",
        "# We separate each context in this file with a blank line\n",
        "# This blank line is used for the implementation of the models\n",
        "\n",
        "# The method used is quite simple:\n",
        "# ctx[answ_start] = B\n",
        "# ctx[answ_start + 1:len(answ)] = I\n",
        "# ctx[else] = O\n",
        "\n",
        "i_twt = twt()\n",
        "\n",
        "def get_data_lines(s):\n",
        "    retval = []\n",
        "    with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-goldp-v1.1-\"+s+\".json\",\"r\") as f:\n",
        "        retval = f.readlines()\n",
        "    return retval\n",
        "\n",
        "def store_result(fname,txt):\n",
        "    with open(\"/content/drive/My Drive/NLP 2020W/data/\"+fname+\".txt\",\"w\") as f:\n",
        "        f.write(txt)\n",
        "\n",
        "def create_tokens(txt):\n",
        "    # create tokens along their position\n",
        "    # output : [(token,pos)]\n",
        "    res = []\n",
        "    tks = list(i_twt.tokenize(txt))\n",
        "    tks_pos = list(i_twt.span_tokenize(txt))\n",
        "    return [(tks[i],tks_pos[i]) for i in range(0,len(tks))]\n",
        "\n",
        "def data2iob(lines,pred):\n",
        "    # create a string of tokens and IOB-tags.\n",
        "    # the token and the tag is separated by a space\n",
        "    # each entry is separated by a new line\n",
        "    #### args\n",
        "    # pred = predicate for filtering result\n",
        "\n",
        "    #so use dumps to parse all lines of input\n",
        "    jdumps = json.dumps(lines)\n",
        "\n",
        "    #and then load the dumps\n",
        "    jdata = json.loads(jdumps)\n",
        "\n",
        "    res = \"\"\n",
        "    ctxs = \"\"\n",
        "    lang = \"\"\n",
        "\n",
        "    # get data\n",
        "    jd = jdata[0]\n",
        "    jd_obj = json.loads(jd)\n",
        "    k_data = jd_obj[\"data\"]\n",
        "    k_version = jd_obj[\"version\"]\n",
        "\n",
        "    # loop over paragraphs that is nested inside k_data\n",
        "    for kd in k_data:\n",
        "        title = kd[\"title\"]\n",
        "        parg = kd[\"paragraphs\"][0]\n",
        "        ctx = parg[\"context\"]\n",
        "        qas = parg[\"qas\"][0]\n",
        "        qas_question = qas[\"question\"]\n",
        "        qas_answers = qas[\"answers\"]\n",
        "        qas_id = qas[\"id\"]\n",
        "        lang = qas_id.split(\"-\")[0]\n",
        "        \n",
        "        # predicate to use for filtering the result\n",
        "        pred_data = {\n",
        "                \"lang\":lang,\n",
        "                \"title\":title,\n",
        "                \"ctx\":ctx\n",
        "                }\n",
        "\n",
        "        answ_start = qas_answers[0][\"answer_start\"]\n",
        "        answ_txt = qas_answers[0][\"text\"]\n",
        "        \n",
        "        tks = None\n",
        "        try:\n",
        "            # we need to try since at least one context is malformed\n",
        "            # which will make span_tokenizer fail\n",
        "            tks = create_tokens(ctx)\n",
        "        except:\n",
        "            print(\"malformed ctx!\")\n",
        "            #print(\"****title=\" + title)\n",
        "            #print(\"****lang=\" + lang)\n",
        "            #print(\"****ctx=\" + ctx)\n",
        "\n",
        "        # loop over tokens\n",
        "        # store each along the IOB tag as a string\n",
        "        # tag and token are seperated by a single space\n",
        "        tag_res = \"\"\n",
        "        if tks != None and pred(pred_data):\n",
        "            ctxs += ctx + \"\\n\"\n",
        "            for (w,pos) in tks:\n",
        "                #tag_res += w + str(pos) + \" \"\n",
        "                tag_res += w + \" \"\n",
        "                if pos[0] == answ_start:\n",
        "                    tag_res += \"B\"\n",
        "                elif pos[0] < answ_start + len(answ_txt) and pos[0] > answ_start:\n",
        "                    tag_res += \"I\"\n",
        "                else:\n",
        "                    tag_res += \"O\"\n",
        "                tag_res += \"\\n\"\n",
        "        \n",
        "            res += tag_res\n",
        "            res += \"\\n\"\n",
        "\n",
        "    return (res,ctxs)\n",
        "\n",
        "\n",
        "\n",
        "#get file\n",
        "\n",
        "exe_set = \"train\"\n",
        "\n",
        "f_lines = get_data_lines(exe_set)\n",
        "\n",
        "# filter predicates\n",
        "def pred_all(pred_data):\n",
        "    return True\n",
        "\n",
        "def pred_en(pred_data):\n",
        "    if pred_data[\"lang\"] == \"english\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def pred_fi(pred_data):\n",
        "    if pred_data[\"lang\"] == \"finnish\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def pred_ar(pred_data):\n",
        "    if pred_data[\"lang\"] == \"arabic\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def pred_ko(pred_data):\n",
        "    if pred_data[\"lang\"] == \"korean\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "#execute\n",
        "(res,ctxs) = data2iob(f_lines,pred_en)\n",
        "store_result(exe_set + \"-eng-iobtagged\",res)\n",
        "\n",
        "(res,ctxs) = data2iob(f_lines,pred_fi)\n",
        "store_result(exe_set + \"-fin-iobtagged\",res)\n",
        "\n",
        "(res,ctxs) = data2iob(f_lines,pred_ar)\n",
        "store_result(exe_set + \"-ara-iobtagged\",res)\n",
        "\n",
        "(res,ctxs) = data2iob(f_lines,pred_ko)\n",
        "store_result(exe_set + \"-kor-iobtagged\",res)\n",
        "\n",
        "print(\"done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BDgkFSZ9Gev"
      },
      "source": [
        "#data_reader\n",
        "\n",
        "import sys\n",
        "import io\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torchcrf import CRF\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
        "from typing import List, Tuple, AnyStr\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "#from data_reader import * \n",
        "\n",
        "def read_data(filename, splitchar = '\\t'):\n",
        "    # read in the data.\n",
        "    # the data is rightly tagged\n",
        "\n",
        "    # Tokenization is not an issue since the data is already listed in tokens (words)\n",
        "    dataset = []\n",
        "    with open(filename) as f:\n",
        "        data = f.read()\n",
        "        samples = data.split('\\n\\n')\n",
        "        for s in samples:\n",
        "            if s:\n",
        "                tok_idx = [t.split(splitchar) for t in s.strip().split('\\n')]\n",
        "                tokens,idx = zip(*tok_idx)\n",
        "                tokens = list(tokens)\n",
        "                idx = list(idx)\n",
        "                dataset.append([tokens,idx])\n",
        "    return dataset\n",
        "\n",
        "# set data\n",
        "def setdata(language):\n",
        "    data = read_data(\"/content/drive/My Drive/NLP 2020W/data/train-\"+language+\"-iobtagged.txt\",splitchar = \" \")\n",
        "\n",
        "    # Divide into train,dev,test and so on.\n",
        "    # The dev-set has been handed out. So data is divided into test and trian\n",
        "    random.shuffle(data)\n",
        "    test_idx = int(len(data) * 0.9)\n",
        "    test_data = data[test_idx:]\n",
        "    train_data = data[:test_idx]\n",
        "    dev_idx = int(len(train_data) * 0.9)\n",
        "    dev_data = train_data[dev_idx:]\n",
        "    train_data = train_data[0:dev_idx]\n",
        "    #dev_data = read_data(\"data/dev-eng-iobtagged.txt\",splitchar = \" \")\n",
        "    #random.shuffle(dev_data)\n",
        "\n",
        "    #print1 = [(t, l) for t,l in zip(train_data[5][0], train_data[5][1])]\n",
        "\n",
        "    # A map of labels, eg. label-vocab.\n",
        "    # label_map = {'B': 0, 'O': 1, 'I': 2}\n",
        "    # where the numbers each field points to, is an index\n",
        "    label_map = {l:i for i,l in enumerate((set([l for s in train_data for l in s[1]]) | set([l for s in dev_data for l in s[1]]) | set([l for s in test_data for l in s[1]])))}\n",
        "    num_labels = len(label_map)\n",
        "    return dev_data, train_data, data, test_data, label_map, num_labels\n",
        "\n",
        "dev_data, train_data, data, test_data, label_map, num_labels = setdata(\"eng\")\n",
        "\n",
        "def load_vectors(fname, vocabulary):\n",
        "    # create vectors for word embeddings. As in the lab we use all of train and dev\n",
        "    # plus most common words word embeddings\n",
        "\n",
        "    # load vectors from pretrained embeddings\n",
        "    # each vector consists of the given word followed by the embedding\n",
        "    # for the pretrained model we use, we have vec length of 1 + 300\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    # define utlity tokens\n",
        "    # the embeddings might clash with pretrained if we are very unlucky?\n",
        "    PAD_vec,PAD_tok = np.random.normal(size=(300,)),'[PAD]'\n",
        "    UNK_vec,UNK_tok = np.random.normal(size=(300,)),'[UNK]'\n",
        "    BOS_vec,BOS_tok = np.random.normal(size=(300,)),'[BOS]'\n",
        "    EOS_vec,EOS_tok = np.random.normal(size=(300,)),'[EOS]'\n",
        "\n",
        "    # final vocabulary is org vocabulary + the 4 tokens below + the most common from pretrained\n",
        "    final_vocab = [PAD_tok, UNK_tok , BOS_tok, EOS_tok]\n",
        "\n",
        "    # final vectors are vectors corresponding to the final vocab\n",
        "    final_vectors = [PAD_vec, UNK_vec, BOS_vec, EOS_vec]\n",
        "\n",
        "    # iterate pretrained word embs\n",
        "    for j,line in enumerate(fin):\n",
        "        word_emb = line.rstrip().split(' ')\n",
        "        if word_emb[0] in vocabulary or len(final_vocab) < 700000:\n",
        "            # check if the current word embedding is in vocab or if the max-cap is reached\n",
        "\n",
        "            # the second condition is the nr of extra tokens to include\n",
        "            # in lab the org vocabsize is 8013, extra is 30000. Hence roughly 20000 extra\n",
        "            # here we have org voc of ~46000, so plus 20000 and we are at 70000\n",
        "            final_vocab.append(word_emb[0])\n",
        "            final_vectors.append(np.array(list(map(float, word_emb[1:]))))\n",
        "    return final_vocab, np.vstack(final_vectors)\n",
        "\n",
        "# Load all vocabulary as words from the training set along the dev set\n",
        "vocabulary = (set([t for s in train_data for t in s[0]]) | set([t for s in dev_data for t in s[0]]))\n",
        "vocabulary, pretrained_embeddings = load_vectors('wiki-news-300d-1M.vec', vocabulary)\n",
        "\n",
        "print2 = len(vocabulary)\n",
        "#print(print2) # = 701152, in lab6 we have vocabsize = 31601\n",
        "\n",
        "\n",
        "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    input_ids = [i[0][0] for i in input_data]\n",
        "    seq_lens = [i[1][0] for i in input_data]\n",
        "    labels = [i[2] for i in input_data]\n",
        "\n",
        "    max_length = max([len(i) for i in input_ids])\n",
        "\n",
        "    input_ids = [(i + [0] * (max_length - len(i))) for i in input_ids]\n",
        "    labels = [(i + [label_map['O']] * (max_length - len(i))) for i in labels]\n",
        "\n",
        "    assert (all(len(i) == max_length for i in input_ids))\n",
        "    assert (all(len(i) == max_length for i in labels))\n",
        "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)\n",
        "\n",
        "def text_to_batch_bilstm(text: List, tokenizer) -> Tuple[List, List]:\n",
        "    \"\"\"\n",
        "    Creates a tokenized batch for input to a bilstm model\n",
        "    :param text: A list of sentences to tokenize\n",
        "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
        "    :return: Tokenized text as well as the length of the input sequence\n",
        "    \"\"\"\n",
        "    # Some light preprocessing\n",
        "    input_ids = [tokenizer.encode(t) for t in text]\n",
        "\n",
        "    return input_ids, [len(ids) for ids in input_ids]\n",
        "\n",
        "class FasttextTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        # create own vocab as dict\n",
        "        # with word as key and enum-id as val\n",
        "        self.vocab = {}\n",
        "        for j,l in enumerate(vocabulary):\n",
        "            self.vocab[l.strip()] = j\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Text is assumed to be tokenized\n",
        "        # Returns enum-id of every token of text\n",
        "        return [self.vocab[t] if t in self.vocab else self.vocab['[UNK]'] for t in text]\n",
        "\n",
        "# This will load the dataset and process it lazily in the __getitem__ function\n",
        "class NERDatasetReader(Dataset):\n",
        "  def __init__(self, dataset, tokenizer):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset[idx]\n",
        "    # Calls the text_to_batch function\n",
        "    input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
        "    labels = [label_map[l] for l in row[1]]\n",
        "    return input_ids, seq_lens, labels\n",
        "\n",
        "reader = NERDatasetReader(dev_data, FasttextTokenizer(vocabulary))\n",
        "dev_dl = DataLoader(reader, batch_size=1, shuffle=False, collate_fn=collate_batch_bilstm)\n",
        "print3 = next(iter(dev_dl))\n",
        "# print(print3) = print of tensors\n",
        "\n",
        "def do_print4():\n",
        "    # res is a print of words, tags and so on\n",
        "    print(dev_data[0])\n",
        "    print(vocabulary[3285],vocabulary[60],vocabulary[2162],vocabulary[4],vocabulary[4484],\n",
        "          vocabulary[524],vocabulary[52],vocabulary[1111],vocabulary[1192],vocabulary[8],\n",
        "          vocabulary[5],vocabulary[683],vocabulary[5204],vocabulary[8],vocabulary[5],\n",
        "          vocabulary[14131],vocabulary[6158],vocabulary[10],vocabulary[3151],vocabulary[6])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsNvGxh1-VMs"
      },
      "source": [
        "#a4_model_crf\n",
        "\n",
        "import sys\n",
        "import io\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torchcrf import CRF\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
        "from typing import List, Tuple, AnyStr\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "# link to lab6:\n",
        "# https://colab.research.google.com/drive/1qy5lgp4jEu-FLDb3w_UIOxnwbk5Fykw1?usp=sharing\n",
        "\n",
        "# we always do this \n",
        "def enforce_reproducibility(seed=42):\n",
        "    # Sets seed manually for both CPU and CUDA\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # For atomic operations there is currently \n",
        "    # no simple way to enforce determinism, as\n",
        "    # the order of parallel operations is not known.\n",
        "    # CUDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # System based\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()\n",
        "\n",
        "#from data_readers import *\n",
        "\n",
        "\"\"\"\n",
        "****Creating the model\n",
        "\n",
        "You'll notice that the BiLSTM model is mostly the same from the text classification and language modeling labs. The differences are:\n",
        "\n",
        "- Instead of taking a softmax/cross-entropy loss using the logits from the BiLSTM, we pass the logits to the pytorch-crf CRF module. The output of this model is the log-likelihood of the entire sequence (for each sequence in the batch). Since our objective is to minimize the loss, we take the negative of the log likelihood as our loss.\n",
        "- There is now a decode function, which passes logits through the CRF to get the most likely tag sequences.\n",
        "\"\"\"\n",
        "# Define the model\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic BiLSTM-CRF network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            pretrained_embeddings: torch.tensor,\n",
        "            lstm_dim: int,\n",
        "            dropout_prob: float = 0.1,\n",
        "            n_classes: int = 2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializer for basic BiLSTM network\n",
        "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
        "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
        "        :param dropout_prob: Dropout probability\n",
        "        :param n_classes: The number of output classes\n",
        "        \"\"\"\n",
        "\n",
        "        # First thing is to call the superclass initializer\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "\n",
        "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
        "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
        "        self.model = nn.ModuleDict({\n",
        "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
        "            'bilstm': nn.LSTM(\n",
        "                pretrained_embeddings.shape[1],\n",
        "                lstm_dim,\n",
        "                2,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_prob,\n",
        "                bidirectional=True),\n",
        "            'ff': nn.Linear(2*lstm_dim, n_classes),\n",
        "            'CRF': CRF(n_classes, batch_first=True)\n",
        "        })\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Initialize the weights of the model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
        "                     list(self.model['ff'].named_parameters())\n",
        "        for n,p in all_params:\n",
        "            if 'weight' in n:\n",
        "                nn.init.xavier_normal_(p)\n",
        "            elif 'bias' in n:\n",
        "                nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, inputs, input_lens, labels = None):\n",
        "        \"\"\"\n",
        "        Defines how tensors flow through the model\n",
        "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
        "        :param input_lens: (b) The length of each input sequence\n",
        "        :param labels: (b) The label of each sample\n",
        "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings (b x sl x edim)\n",
        "        embeds = self.model['embeddings'](inputs)\n",
        "\n",
        "        # Pack padded: This is necessary for padded batches input to an RNN\n",
        "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
        "            embeds,\n",
        "            input_lens,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass the packed sequence through the BiLSTM\n",
        "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
        "\n",
        "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
        "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Get emissions (b x seq_len x n_classes)\n",
        "        emissions = self.model['ff'](lstm_out)\n",
        "        outputs = (emissions,)\n",
        "        if labels is not None:\n",
        "            mask = (inputs != 0)\n",
        "            # log-likelihood from the CRF\n",
        "            log_likelihood = self.model['CRF'](emissions, labels, mask=mask, reduction='token_mean')\n",
        "            outputs = (-log_likelihood,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def decode(self, emissions, mask):\n",
        "      \"\"\"\n",
        "      Given a set of emissions and a mask, decode the sequence\n",
        "      \"\"\"\n",
        "      return self.model['CRF'].decode(emissions, mask=mask)\n",
        "\n",
        "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
        "  \"\"\"\n",
        "  Evaluates the model on the given dataset\n",
        "  :param model: The model under evaluation\n",
        "  :param valid_dl: A `DataLoader` reading validation data\n",
        "  :return: The accuracy of the model on the dataset\n",
        "  \"\"\"\n",
        "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like \n",
        "  # layer normalization and dropout\n",
        "  model.eval()\n",
        "  labels_all = []\n",
        "  logits_all = []\n",
        "  tags_all = []\n",
        "\n",
        "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      seq_lens = batch[1]\n",
        "      labels = batch[2]\n",
        "\n",
        "      _, logits = model(input_ids, seq_lens, labels=labels)\n",
        "      mask = (input_ids != 0)\n",
        "      labels_all.extend([l for seq,samp in zip(list(labels.detach().cpu().numpy()), input_ids) for l,i in zip(seq,samp) if i != 0])\n",
        "      logits_all.extend(list(logits.detach().cpu().numpy()))\n",
        "      \n",
        "      tags = model.decode(logits, mask)\n",
        "      tags_all.extend([t for seq in tags for t in seq])\n",
        "    \n",
        "    # the last param is for treating warning of none present labels/tags\n",
        "    # https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
        "    P, R, F1, _ = precision_recall_fscore_support(labels_all, tags_all, average='macro',labels=np.unique(tags_all))\n",
        "\n",
        "    return F1\n",
        "\n",
        "def train(\n",
        "    model_name,\n",
        "    model: nn.Module, \n",
        "    train_dl: DataLoader, \n",
        "    valid_dl: DataLoader, \n",
        "    optimizer: torch.optim.Optimizer, \n",
        "    n_epochs: int, \n",
        "    device: torch.device,\n",
        "    scheduler=None\n",
        "):\n",
        "  \"\"\"\n",
        "  The main training loop which will optimize a given model on a given dataset\n",
        "  :param model: The model being optimized\n",
        "  :param train_dl: The training dataset\n",
        "  :param valid_dl: A validation dataset\n",
        "  :param optimizer: The optimizer used to update the model parameters\n",
        "  :param n_epochs: Number of epochs to train for\n",
        "  :param device: The device to train on\n",
        "  :return: (model, losses) The best model and the losses per iteration\n",
        "  \"\"\"\n",
        "\n",
        "  # Keep track of the loss and best accuracy\n",
        "  losses = []\n",
        "  learning_rates = []\n",
        "  best_f1 = 0.0\n",
        "\n",
        "  # Iterate through epochs\n",
        "  for ep in range(n_epochs):\n",
        "\n",
        "    loss_epoch = []\n",
        "\n",
        "    #Iterate through each batch in the dataloader\n",
        "    for batch in tqdm(train_dl):\n",
        "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on \n",
        "      # things like dropout and layer normalization\n",
        "      model.train()\n",
        "\n",
        "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
        "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
        "      # zero them out\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Place each tensor on the GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      seq_lens = batch[1]\n",
        "      labels = batch[2]\n",
        "\n",
        "      # Pass the inputs through the model, get the current loss and logits\n",
        "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
        "      losses.append(loss.item())\n",
        "      loss_epoch.append(loss.item())\n",
        "      \n",
        "      # Calculate all of the gradients and weight updates for the model\n",
        "      loss.backward()\n",
        "\n",
        "      # Optional: clip gradients\n",
        "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Finally, update the weights of the model\n",
        "      optimizer.step()\n",
        "      if scheduler != None:\n",
        "        scheduler.step()\n",
        "        learning_rates.append(scheduler.get_last_lr()[0])\n",
        "        \n",
        "    #gc.collect()\n",
        "\n",
        "    # Perform inline evaluation at the end of the epoch\n",
        "    f1 = evaluate(model, valid_dl)\n",
        "    print(f'Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
        "\n",
        "    # Keep track of the best model based on the accuracy\n",
        "    if f1 > best_f1:\n",
        "      torch.save(model.state_dict(), model_name)\n",
        "      best_f1 = f1\n",
        "        #gc.collect()\n",
        "\n",
        "  return losses, learning_rates\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # Create the optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    scheduler = CyclicLR(optimizer, base_lr=0., max_lr=lr, step_size_up=1, step_size_down=len(train_dl)*n_epochs, cycle_momentum=False)\n",
        "\n",
        "    losses, learning_rates = train(model_name,model, train_dl, valid_dl, optimizer, n_epochs, device, scheduler)\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "    plt.plot(learning_rates)\n",
        "    plt.show()\n",
        "\n",
        "arr_languages = [\"eng\", \"fin\", \"ara\", \"kor\"]\n",
        "for language in arr_languages: \n",
        "  dev_data, train_data, _, _, _, _ = setdata(language)\n",
        "  # hyperparams\n",
        "  model_name = language+\"_model_crf\"\n",
        "  lstm_dim = 128\n",
        "  dropout_prob = 0.1\n",
        "  batch_size = 8\n",
        "  lr = 1e-2\n",
        "  n_epochs = 10\n",
        "\n",
        "  device = torch.device(\"cpu\")\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"has cuda\")\n",
        "      device = torch.device(\"cuda\")\n",
        "\n",
        "  # Create the model\n",
        "  model = BiLSTM_CRF(\n",
        "      pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
        "      lstm_dim=lstm_dim, \n",
        "      dropout_prob=dropout_prob, \n",
        "      n_classes=len(label_map)\n",
        "    ).to(device)\n",
        "\n",
        "  # Create the dataset readers\n",
        "  train_dataset = NERDatasetReader(train_data, FasttextTokenizer(vocabulary))\n",
        "  # dataset loaded lazily with N workers in parallel\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  valid_dataset = NERDatasetReader(dev_data, FasttextTokenizer(vocabulary))\n",
        "  valid_dl = DataLoader(valid_dataset, batch_size=len(dev_data), collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  train_model()\n",
        "\n",
        "  model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "  # Evaluate\n",
        "  dev_dataset = NERDatasetReader(dev_data, FasttextTokenizer(vocabulary))\n",
        "  dev_dl = DataLoader(dev_dataset, batch_size=len(dev_data), collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  test_dataset = NERDatasetReader(test_data, FasttextTokenizer(vocabulary))\n",
        "  #test_dl = DataLoader(test_dataset, batch_size=len(test_data), collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "  test_dl = DataLoader(test_dataset, batch_size=1, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  print5 = evaluate(model, test_dl)\n",
        "  print(print5)\n",
        "\n",
        "\n",
        "def last_part():\n",
        "    model.eval()\n",
        "    ex = 4\n",
        "    samples = [b.to(device) for b in next(iter(c_dl))]\n",
        "\n",
        "# Get the emissions. These are basically p(y|x) for each token x,\n",
        "# which will be input to the CRF a decoded with the help of p(y_t|y_{t-1})\n",
        "    (emissions,) = model(samples[0], samples[1])\n",
        "    mask = (samples[0] != 0)\n",
        "\n",
        "    tags = model.decode(emissions, mask)\n",
        "\n",
        "    id_to_label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "    print10 = [(tok,id_to_label[tag]) for tok,tag in zip(test_data[ex][0], tags[ex])]\n",
        "    print(print10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQTJDcd1-YZu"
      },
      "source": [
        "#a4_model_noncrf\n",
        "\n",
        "import sys\n",
        "import io\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torchcrf import CRF\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
        "from typing import List, Tuple, AnyStr\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "# link to lab6:\n",
        "# https://colab.research.google.com/drive/1qy5lgp4jEu-FLDb3w_UIOxnwbk5Fykw1?usp=sharing\n",
        "\n",
        "# we always do this \n",
        "def enforce_reproducibility(seed=42):\n",
        "    # Sets seed manually for both CPU and CUDA\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # For atomic operations there is currently \n",
        "    # no simple way to enforce determinism, as\n",
        "    # the order of parallel operations is not known.\n",
        "    # CUDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # System based\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()\n",
        "\n",
        "#from data_readers import *\n",
        "\n",
        "\"\"\"\n",
        "Creating the model\n",
        "This differs from \n",
        "\"\"\"\n",
        "lstm_dim = 128\n",
        "# Define the model\n",
        "class BiLSTMNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic BiLSTM network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            pretrained_embeddings: torch.tensor,\n",
        "            lstm_dim: int,\n",
        "            dropout_prob: float = 0.1,\n",
        "            n_classes: int = 2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializer for basic BiLSTM network\n",
        "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
        "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
        "        :param dropout_prob: Dropout probability\n",
        "        :param n_classes: The number of output classes\n",
        "        \"\"\"\n",
        "\n",
        "        # First thing is to call the superclass initializer\n",
        "        super(BiLSTMNetwork, self).__init__()\n",
        "\n",
        "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
        "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
        "        self.model = nn.ModuleDict({\n",
        "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
        "            'bilstm': nn.LSTM(\n",
        "                pretrained_embeddings.shape[1],\n",
        "                lstm_dim,\n",
        "                2,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_prob,\n",
        "                bidirectional=True),\n",
        "            'ff': nn.Linear(2*lstm_dim, n_classes)\n",
        "        })\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Initialize the weights of the model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
        "                     list(self.model['ff'].named_parameters())\n",
        "        for n,p in all_params:\n",
        "            if 'weight' in n:\n",
        "                nn.init.xavier_normal_(p)\n",
        "            elif 'bias' in n:\n",
        "                nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, inputs, input_lens, labels = None):\n",
        "        \"\"\"\n",
        "        Defines how tensors flow through the model\n",
        "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
        "        :param input_lens: (b) The length of each input sequence\n",
        "        :param labels: (b) The label of each sample\n",
        "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings (b x sl x edim)\n",
        "        embeds = self.model['embeddings'](inputs)\n",
        "\n",
        "        # Pack padded: This is necessary for padded batches input to an RNN\n",
        "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
        "            embeds,\n",
        "            input_lens,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass the packed sequence through the BiLSTM\n",
        "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
        "\n",
        "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
        "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
        "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
        "\n",
        "        # Get logits (batch_size x seq_len x n_labels)\n",
        "        logits = self.model['ff'](lstm_out)\n",
        "        #logits = self.model[\"ff\"](ff_in)\n",
        "        outputs = (logits,)\n",
        "        if labels is not None:\n",
        "            # Xentropy loss - this can only be used with a target of dim = 1\n",
        "            # Hence we flatten\n",
        "            loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "            # We flatten labels to have shape (batch_size * seq_len)\n",
        "            labels_flat = labels.flatten()\n",
        "            # We flatten logits to have shape (batch_size * seq_len x n_labels)\n",
        "            logits_flat = logits.flatten(end_dim=1)\n",
        "            loss = loss_fn(logits_flat, labels_flat)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    logits = np.array(logits)\n",
        "    labels = np.array(labels)\n",
        "    return np.sum(np.argmax(logits, axis=-1) == labels).astype(np.float32) / float(labels.shape[0])\n",
        "\n",
        "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
        "  \"\"\"\n",
        "  Evaluates the model on the given dataset\n",
        "  :param model: The model under evaluation\n",
        "  :param valid_dl: A `DataLoader` reading validation data\n",
        "  :return: The accuracy of the model on the dataset\n",
        "  \"\"\"\n",
        "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like \n",
        "  # layer normalization and dropout\n",
        "  model.eval()\n",
        "  labels_all = []\n",
        "  logits_all = []\n",
        "\n",
        "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      seq_lens = batch[1]\n",
        "      labels = batch[2]\n",
        "\n",
        "      _, logits = model(input_ids, seq_lens, labels=labels)\n",
        "\n",
        "      logits = logits.flatten(end_dim=1)\n",
        "      labels = labels.flatten()\n",
        "\n",
        "      labels_all.extend(list(labels.detach().cpu().numpy()))\n",
        "      logits_all.extend(list(logits.detach().cpu().numpy()))\n",
        "    acc = accuracy(logits_all, labels_all)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module, \n",
        "    train_dl: DataLoader, \n",
        "    valid_dl: DataLoader, \n",
        "    optimizer: torch.optim.Optimizer, \n",
        "    n_epochs: int, \n",
        "    device: torch.device\n",
        "):\n",
        "  \"\"\"\n",
        "  The main training loop which will optimize a given model on a given dataset\n",
        "  :param model: The model being optimized\n",
        "  :param train_dl: The training dataset\n",
        "  :param valid_dl: A validation dataset\n",
        "  :param optimizer: The optimizer used to update the model parameters\n",
        "  :param n_epochs: Number of epochs to train for\n",
        "  :param device: The device to train on\n",
        "  :return: (model, losses) The best model and the losses per iteration\n",
        "  \"\"\"\n",
        "\n",
        "  # Keep track of the loss and best accuracy\n",
        "  losses = []\n",
        "  best_acc = 0.0\n",
        "\n",
        "  # Iterate through epochs\n",
        "  for ep in range(n_epochs):\n",
        "\n",
        "    loss_epoch = []\n",
        "\n",
        "    #Iterate through each batch in the dataloader\n",
        "    for batch in tqdm(train_dl):\n",
        "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on \n",
        "      # things like dropout and layer normalization\n",
        "      model.train()\n",
        "\n",
        "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
        "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
        "      # zero them out\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Place each tensor on the GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids = batch[0]\n",
        "      seq_lens = batch[1]\n",
        "      labels = batch[2]\n",
        "\n",
        "      # Pass the inputs through the model, get the current loss and logits\n",
        "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
        "      losses.append(loss.item())\n",
        "      loss_epoch.append(loss.item())\n",
        "      \n",
        "      # Calculate all of the gradients and weight updates for the model\n",
        "      loss.backward()\n",
        "\n",
        "      # Optional: clip gradients\n",
        "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Finally, update the weights of the model\n",
        "      optimizer.step()\n",
        "      #gc.collect()\n",
        "\n",
        "    # Perform inline evaluation at the end of the epoch\n",
        "    acc = evaluate(model, valid_dl)\n",
        "    print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
        "\n",
        "    # Keep track of the best model based on the accuracy\n",
        "    best_model = model.state_dict()\n",
        "    if acc > best_acc:\n",
        "      best_model = model.state_dict()\n",
        "      best_acc = acc\n",
        "        #gc.collect()\n",
        "\n",
        "  model.load_state_dict(best_model)\n",
        "  return model, losses\n",
        "\n",
        "def train_model(model):\n",
        "    # Create the optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train\n",
        "    model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n",
        "\n",
        "    # save best model configuration\n",
        "    torch.save(model.state_dict(), model_name)\n",
        "    \n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "arr_languages = [\"eng\", \"fin\", \"ara\", \"kor\"]\n",
        "for language in arr_languages: \n",
        "  dev_data, train_data, _, _, _, _ = setdata(language)\n",
        "  # hyperparams\n",
        "  model_name = \"model_\"+language+\"_noncrf\"\n",
        "  batch_size = 8\n",
        "  lr = 2e-4\n",
        "  n_epochs = 10\n",
        "\n",
        "\n",
        "  device = torch.device(\"cpu\")\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"has cuda\")\n",
        "      device = torch.device(\"cuda\")\n",
        "\n",
        "  # Create the model\n",
        "  model = BiLSTMNetwork(\n",
        "      pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
        "      lstm_dim=lstm_dim, \n",
        "      dropout_prob=0.1, \n",
        "      n_classes=len(label_map)\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "  # Create the dataset readers\n",
        "  train_dataset = NERDatasetReader(train_data, FasttextTokenizer(vocabulary))\n",
        "  # dataset loaded lazily with N workers in parallel\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  valid_dataset = NERDatasetReader(dev_data, FasttextTokenizer(vocabulary))\n",
        "  valid_dl = DataLoader(valid_dataset, batch_size=len(dev_data), collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  train_model(model)\n",
        "\n",
        "\n",
        "  model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  test_dataset = NERDatasetReader(test_data, FasttextTokenizer(vocabulary))\n",
        "  #test_dl = DataLoader(test_dataset, batch_size=len(test_data), collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "  test_dl = DataLoader(test_dataset, batch_size=1, collate_fn=collate_batch_bilstm, num_workers=8)\n",
        "\n",
        "  print5 = evaluate(model, test_dl)\n",
        "  print(print5)\n",
        "\n",
        "\n",
        "  # load best model configuration and run on test data\n",
        "  model.load_state_dict(torch.load(model_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}