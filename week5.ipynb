{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAD9srW2kwH"
      },
      "source": [
        "#### Mount Google Drive (datasets are stored there):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP4L-yFeYURJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtaVl79pzm54"
      },
      "source": [
        "#### Load relevant questions from dataset, also download nltk word tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7lUS_on3q1x"
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# we use nltk to tokenize multi-lingual sequences\n",
        "def tokenize_at_word_level(input):\n",
        "  return nltk.tokenize.word_tokenize(input)\n",
        "\n",
        "# define supported languages\n",
        "supported_languages = ['english', 'arabic', 'finnish', 'korean']\n",
        "\n",
        "binary_labels = ['YES', 'NO']\n",
        "\n",
        "# helper function to return all relevant properties\n",
        "def relevant_properties(question):\n",
        "  return {\n",
        "    \"question\": question['question_text'],\n",
        "    \"document\": question['document_plaintext'],\n",
        "    \"answer\": question['annotations'][0]['yes_no_answer'].upper()\n",
        "  }\n",
        "\n",
        "# helper function to import questions from given file\n",
        "def import_questions(file):\n",
        "  questions = {}\n",
        "\n",
        "  for lang in supported_languages:\n",
        "    questions[lang] = []\n",
        "\n",
        "  for line in file:\n",
        "    question = json.loads(line)\n",
        "    lang = question['language']\n",
        "\n",
        "    # add question if dict contains key for it and it has yes/no answer \n",
        "    if (lang in list(questions.keys()) and\n",
        "        relevant_properties(question)['answer'] in binary_labels\n",
        "      ):\n",
        "      questions[lang].append(relevant_properties(question))\n",
        "\n",
        "  return questions\n",
        "\n",
        "# questions used for training our classifier(s)\n",
        "with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-v1.0-train.jsonl\") as file:\n",
        "  train_questions = import_questions(file)\n",
        "\n",
        "# questions used to evaluate our classifier(s)\n",
        "with open(\"/content/drive/My Drive/NLP 2020W/tydiqa-v1.0-dev.jsonl\") as file:\n",
        "  dev_questions = import_questions(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPUoty6YPmNX"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "720iT15zsdVC"
      },
      "source": [
        "# https://huggingface.co/transformers/model_doc/marian.html\n",
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# we want to translate from english to finnish and arabic (no pre-trained korean model available)\n",
        "translate_to = ['fi', 'ar']\n",
        "translated_questions = {}\n",
        "\n",
        "models = {}\n",
        "tokenizers = {}\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "for lang in translate_to:\n",
        "  translated_questions[lang] = []\n",
        "  model_name = 'Helsinki-NLP/opus-mt-en-' + lang\n",
        "\n",
        "  models[lang] = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "  tokenizers[lang] = MarianTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr7TjDupSwuy"
      },
      "source": [
        "%%time\n",
        "\n",
        "# we use nltk to tokenize multi-lingual sentences\n",
        "def tokenize_at_sentence_level(input):\n",
        "  return nltk.tokenize.sent_tokenize(input)\n",
        "\n",
        "# translate all english questions to finnish and arabic\n",
        "for q in train_questions['english']:\n",
        "  for lang in translate_to:\n",
        "    question_batches = tokenizers[lang].prepare_seq2seq_batch([q['question']]).to(device)\n",
        "\n",
        "    question_translated = [\n",
        "      tokenizers[lang].decode(t, skip_special_tokens=True)\n",
        "      for t in models[lang].generate(**question_batches)\n",
        "    ]\n",
        "\n",
        "    # batch input, translate to given model and decode the output\n",
        "    doc_translated = []\n",
        "\n",
        "    # tokenize the documents using nltk\n",
        "    for sent in tokenize_at_sentence_level(q['document']):\n",
        "      doc_batches = tokenizers[lang].prepare_seq2seq_batch([sent]).to(device)\n",
        "\n",
        "      doc_translated.append([\n",
        "        tokenizers[lang].decode(t, skip_special_tokens=True)\n",
        "        for t in models[lang].generate(**doc_batches)\n",
        "      ])\n",
        "    \n",
        "    translated_questions[lang].append({\n",
        "      'question': question_translated,\n",
        "      'document': \" \".join([y for x in doc_translated for y in x])\n",
        "    })\n",
        "\n",
        "    print(\"{}/{}\".format(len(translated_questions['ar']) + len(translated_questions['fi']), 2 * len(train_questions['english'])))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAjpOx2y_nKr"
      },
      "source": [
        "translated_questions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}